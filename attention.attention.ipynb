{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp attention.attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention\n",
    "### Introduction ### \n",
    "I this blog post I wanna to introduce anttention. So, I'm gonna deviate from what I've seen as the usual introduction to anttention. I think it's important to see things in historical context. I'm gonna try and explain the paper which introduced anttention, the context in which it was introduced. Further, where most tutorials focus on the vector representation of the anttention mechanism I'm gonna show in the fewest possible lines of code the core of the anttention mechanism. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Machine Translation by Jointly Learning to Align and Translate #### \n",
    "\n",
    "This paper introduces anttention in the context of sequence-to-sequence modelling. The paper describes the status of NMT models and the problem the authors are trying to solve with anttention: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper states that an issue with the encoder-decoder approach is that\n",
    "\n",
    "`the last hidden state that the encoder outputs and is used for the translation cannot contain all the information necessary for the entire translation.`\n",
    "\n",
    "So, understanding this forms the core of the problem is trying to solve and it's worth taking a minute to really understand what's going on. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper introduces a novel mechanism to bring about improvements to the task of sequence to sequence modelling. I think it's worth taking a minute to really drill down on the setup, the problem and then delve into how anttention is gonna solve that problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/seq2seq1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not aware of how an RNN functions you need to stop here understand first. If you're not comfortable with how a RNN functions things are not going to be as clear. As the rest of you know an RNN processes a sequence with a time delay. After processing each new token in the input it creates a hidden state which is a vector representation of the sequence till the last input. \n",
    "\n",
    "So, from the diagram `h3` represents the sequence till `morgen`. \n",
    "\n",
    "The seq-to-seq setup is usually used for translation between languages. So, the last hidden state which represents the entire sequence becomes the initial hidden state for another RNN that is going to be responsible for outputting the translated sequence.  \n",
    "\n",
    "This is the basic setup for a seq-to-seq model. Now, let's take a look at the paper describing the possibe problem with this steup.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Most  of  the  proposed  neural  machine  translation  models  belong  to  a  family  of encoder–decoders, with an encoder and a decoder for each language, or involve a language-specific encoder applied to each sentence whose outputs are then compared.  An encoder neural network reads and encodes a source sen-tence into a fixed-length vector. A decoder then outputs a translation from the encoded vector. The whole encoder–decoder system, which consists of the encoder and the decoder for a language pair,is jointly trained to maximize the probability of a correct translation given a source sentence.\n",
    "\n",
    ">A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector.  This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus. Choet al.(2014b) showed that indeed the performance of a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want highlight what I feel is the important part of this description: \n",
    "\n",
    "`A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector.`\n",
    "\n",
    "What the paper is saying is that the vector `z` might not contain enough information to remember everything it has been fed and might forget what it was fed in the earlier part of the sentence. So, taking the example given in the diagram if `z` is small enough it might not remember `guten` after consuming `morgen`. The size of the vector `z` is in our hands remember. Though I have not seen any studies (I'm sure they exist) comparing the memory capacity of a vector with the size of the vector. For all purposes a vector of size 10 will be able to remember this sequence but as sequences get larger we will hit a ceiling. \n",
    "\n",
    "So, we want to (when called upon) remember whichever parts of the input sequence that we like. Well, as mentioned earlier we have the hidden state for every token we processed. If we want to use all the hidden states that are available to us we need answer few basic questions: \n",
    "1. How do we know which hidden state to pass to the decoder? \n",
    "2. If I need information from multiple hidden states how do I go about using them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter: Anttention #### \n",
    "\n",
    "When we're using anttention, the first input that the decoder recieves is not the last hidden state of the encoder RNN but the <start> token for the translation. Using this token I want to extract information from the given hidden states of the encoder. Given the <stat> token we know we want information for the initial set of hidden states. It might be the first few hidden states even. \n",
    "    \n",
    "Here, I would say we're ready to look at how the anttention mechanism functions as we have descibed in plenty detail the two inputs anttention requires: \n",
    "1. A list of vectors from which I want to extract some information. \n",
    "2. A vector which is gonna help me gauage what information I want to extract. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = nn.Linear((ENC_HID_DIM * 2) + DEC_HID_DIM, DEC_HID_DIM)\n",
    "v = nn.Linear(DEC_HID_DIM, 1, bias = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here I'm going to create an artifical torch vectors to step through the mechanism. I would encourage you to go through the complete implementation of the paper once you're done here. I am attaching a notebook which as the complete implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In the first step, this is gonna be the <start> token\n",
    "# Shape: (batch_size,decoder_hidden_dimension)\n",
    "# batch_size: Num of sequences we're processing\n",
    "# decoder_hidden_dimension: hidden state dimension of the decoder RNN\n",
    "hidden = torch.randn(1,512)\n",
    "\n",
    "# List of vectors (Encoder Outputs)\n",
    "# Here the shape is (src_len,batch_size,encoder_hidden_dimension)\n",
    "# src_len: Length of the source sequence\n",
    "# batch_size: number of sequences I'm processing in one go\n",
    "# encoder_hidden_dimension: size of the hidden states in the encoder\n",
    "encoder_outputs = torch.randn(5, 1, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This is what the actual processing is gonna look like. I have taken this code from the `forward` method of \n",
    "# the anttention class. I'm going to be printing the dimensions/ values of vaiables to keep track of what's\n",
    "# happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "batch_size = encoder_outputs.shape[1]\n",
    "src_len = encoder_outputs.shape[0]\n",
    "print(batch_size)\n",
    "print(src_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "#repeat decoder hidden state src_len times\n",
    "\n",
    "# I'm going to perform an operation between the <start> token and each encoder outputs. I want to see how much\n",
    "# information does each vector contain which will help me predict the next output. Thus, to perform such an \n",
    "# operation I'm creating duplicates of the <start> token for each encoder output. \n",
    "\n",
    "print(hidden.shape)\n",
    "hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1024])\n",
      "torch.Size([1, 5, 1024])\n"
     ]
    }
   ],
   "source": [
    "# This is just re-arranging the encoder outputs so for my operation \n",
    "print(encoder_outputs.shape)\n",
    "encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "print(encoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 5, 1024])\n",
      "torch.Size([1, 5, 1536])\n"
     ]
    }
   ],
   "source": [
    "# This is the first step of attention. I'm concatting my <start> token vector and one encoder dimension \n",
    "# vector. Gonna do this with every encoder output vector. \n",
    "\n",
    "print(hidden.shape)\n",
    "print(encoder_outputs.shape)\n",
    "concat_hidden_encoder = torch.cat((hidden, encoder_outputs), dim = 2)\n",
    "print(concat_hidden_encoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# `attn` is just a linear transformation. This will convert my vectors of size 1536 to 512. Simple \n",
    "# Matix Multiplication. \n",
    "energy = torch.tanh(attn(concat_hidden_encoder)) \n",
    "print(energy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another Linear Transformation, converting vectors of size 512 to 1. \n",
    "v(energy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "attention = v(energy).squeeze(2)\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the variable attention I now have a score which is a measure how useful each encoder output vector will \n",
    "# be in determining the next output. Lastly, I want to normalize the scores. I'm gonna do this via softmax. \n",
    "# This is the same softmax which is used in classification. \n",
    "encoder_softmax = F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2748, 0.1554, 0.1157, 0.1979, 0.2562], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can see this is a like the classification probability distribution which sums up to 1. Here, \n",
    "# they can be seen as weights describing the importance of each encoder outut vector. \n",
    "encoder_softmax[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging\n",
    "encoder_softmax = encoder_softmax.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_softmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what I was after. The softmaxxed vector gave the the weighatge of each encoder output in determining\n",
    "# the next output. It's found that when I multiply each encoder output with its weightage and add them all up\n",
    "# I get the information I was looking for from the initial set of encoder outputs. \n",
    "\n",
    "# This particular operation is in the Decoder of the Seq-to-Seq model. \n",
    "weighted = torch.bmm(encoder_softmax, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1024])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together, this is what the attention class looks like: \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "#         import pdb;pdb.set_trace()\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have tried to cover the very core of the attention mechanism. Before moving forward I would highly reccomend to go through the entire implementaton here. I have added several debuggers (pdb) in the Encoder, Decoder, Attention and the final model. \n",
    "\n",
    "To summarise what attention does, given the input of a list of vectors and another vector, attention gives a weightage which signifies the dependence of the vector on each vector in the list of vectors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
