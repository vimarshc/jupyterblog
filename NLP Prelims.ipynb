{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NLP ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello Everyone! \n",
    "Welcome to the first video of this lecture series on Attention. \n",
    "\n",
    "Before we start our discussion on Attention we're gonna breifly discuss how Deep Learning is used in Natural Language Processing to establish a base line of terminologies and to discuss certain concepts that form the bedrock of Deep Learning in NLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like every other usage of Deep Learning models, in NLP we use different kinds of neural networks to create vectorized representations of our input. For any NLP task to create vector space representation text we first start with representing every word in our dataset with a randomly initialized vector. In PyTorch, the [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) class is used to create a mapping between all words in our dataset and a fixed length vector (a tensor of single dimension). Here's an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3612,  0.4019,  0.7331]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3)\n",
    "# Here I've created a mapping where my vocab size is 10 \n",
    "# and the length of my fixed length vector is 3. \n",
    "\n",
    "input = torch.LongTensor([[9]])\n",
    "# Each word in our dataset is given a numerical ID. \n",
    "embedding(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one of the objectives of this video was to help you the viewer develop a habit to look at source code. Keeping that in mind let's take a brief detour and see what the internals of the Embedding class look like and try to recreate sections of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "weight = Parameter(torch.Tensor(10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.4227, -1.5219,  0.9717],\n",
       "        [-1.1892,  0.0214, -0.5868],\n",
       "        [ 2.2857,  1.0582,  0.0106],\n",
       "        [-0.9125, -0.0913, -0.2314],\n",
       "        [-1.4316,  0.5261, -0.6858],\n",
       "        [-0.3281, -0.9034,  0.9479],\n",
       "        [-1.0346,  1.0548, -0.7614],\n",
       "        [-0.5806,  1.0494, -0.1831],\n",
       "        [-1.4869,  0.1978,  0.6518],\n",
       "        [ 1.1752, -0.7529,  1.2326]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal_(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.6990e-26,  3.0871e-41, -2.6363e-29],\n",
       "        [ 4.5593e-41,  2.1449e-02,  5.8680e-01],\n",
       "        [ 2.2857e+00,  1.0582e+00,  1.0593e-02],\n",
       "        [ 9.1253e-01,  9.1309e-02,  2.3141e-01],\n",
       "        [ 1.4316e+00,  5.2611e-01,  6.8577e-01],\n",
       "        [ 3.2810e-01,  9.0337e-01,  9.4789e-01],\n",
       "        [ 1.0346e+00,  1.0548e+00,  7.6137e-01],\n",
       "        [ 5.8065e-01,  1.0494e+00,  1.8311e-01],\n",
       "        [ 1.4869e+00,  1.9779e-01,  6.5181e-01],\n",
       "        [ 1.1752e+00,  7.5294e-01,  1.2326e+00]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another library that we're gonna be actively looking at is the [transformers](https://huggingface.co/transformers/) library. This is a library build on top of PyTorch which provides a lot inbuilt functionality for NLP tasks. Here, I've created the `embeddify` func that uses that library to return the vector space representations for every word passed in a sentence as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-222-4b542d131dd7>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return embeddings(torch.tensor(torch.tensor(tokenizer(text)[\"input_ids\"]).view(1,_len))), tokenizer.convert_ids_to_tokens(token_ids)\n"
     ]
    }
   ],
   "source": [
    "a, b = embeddify('The attention mechanism was invented in 2015')\n",
    "# We will look at the internals of this function at some other point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape\n",
    "# (batch_size,seq_len, token_vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'attention', 'mechanism', 'was', 'invented', 'in', '2015']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`a` is a 3 dimensional tensor. Lets take a look at what each of those dimensions mean: \n",
    "\n",
    "**batch_size** : This refers to the number of sentences being represented by the tensor. Usually when I'm training or evaluating a model I'm gonna be passing multiple sentences. \n",
    "\n",
    "**seq_len**: This refers to the number of words in my sentence. Here, my sentence is made up of 7 words.  Each entity is further represented by a vector of size 768. \n",
    "\n",
    "**token_vector_size**: This is the length of the fixed length vector representing every word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-222-4b542d131dd7>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return embeddings(torch.tensor(torch.tensor(tokenizer(text)[\"input_ids\"]).view(1,_len))), tokenizer.convert_ids_to_tokens(token_ids)\n"
     ]
    }
   ],
   "source": [
    "# rnn_example_seq_tensor, seq = embeddify('This person is a good person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'attention', 'mechanism', 'was', 'invented', 'in', '2015']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_example_hidden = torch.zeros(1,1,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_rnn = nn.RNN(768,768,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs,last_encoder_output = example_rnn(a,rnn_example_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, you can look at a as a sequence of vectors representing our sentence. Given such input  [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)s are used to generate a vector of fixed size. Here, `example_rnn` is an RNN when takes as input a sequence of vectors which are 768 in size and to represent the sequence of tokens uses a vector of size 768.\n",
    "\n",
    "\n",
    "How a RNN works is that it starts off with fixed length vector, which is referred to as the hidden state (rnn_example_hidden) processes the input sequentially. At each step it performs an operation to merge a vector  with the hidden state. \n",
    "\n",
    "`last_encoder_output` represents the entire sequence in `b` and was created after merging the last vector/word with the hidden state. `encoder_outputs` is a list of \"hidden states\" generated after merging each vector of the hidden state. You can see that there are 7 entities in `encoder_outputs`. \n",
    "\n",
    "> **encoder_outputs[0]** represents the merging of **_rnn_example_hidden_** and the word **_The_**\n",
    "\n",
    "> **encoder_outputs[1]** represents the merging of **_encoder_outputs[0]_** and the word **_attention_**\n",
    "\n",
    "So and and so forth. Lastly, encoder_outputs[-1] represents the entire sequence.\n",
    "\n",
    "\n",
    "Now, a discussion of the internals of the RNN module is beyond the scope of this article. For a more in depth introduction to RNNs I would first suggest [this](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) tutorial by [Andrej Karpathy](https://karpathy.ai/) and follow that up with [this](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) discussion by Chris Olah. But it's worth taking a moment to discuss one particular drawback of the RNN. The RNN does not do a very good job of representing long sequences very well and only remembers things near the corresponding input. So, a consequence of that in our case will be: \n",
    "\n",
    "> encoder_outputs[-1] does a good job of remebering the information near the token `2015` but might not represent earlier tokens in the sequence very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Humble Linear Layer ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/1layer.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk a bit about the humble Linear Layer. Now, if any of you have trained a neural network chances are you've stacked up a few linear layers and covered them with activation functions to perform some task. However, for the longest time I did not understand the intuition behind them. The idea is subtle and it has got to do with visualizing what a linear layer does. \n",
    "\n",
    "A Linear layer performs a `Linear Transformations`. Linear Transformations (though they have certain properties attached to them) perform the action of stretching and squishing space it is applied upon. The properties restrict the kind of stretching and squishing that can be done. In essence, the properties translate to prarllel lines should remain parallel.\n",
    "\n",
    "A Linear Transformation maps a particular vector to a different space and it does that by mapping a vector to a space where the i-cap and j-cap of that mapped space are not (1,0) and (0,1) but a different set of vectors. These vectors are represented by the weights of our linear layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin1 = nn.Linear(2,2, bias=False)\n",
    "# Here I have created a Linear Layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6971,  0.6101],\n",
       "        [ 0.2710,  0.0192]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin1.weight\n",
    "# For this LT i-cap is at [-0.6971,  0.2710]\n",
    "#          and j-cap is at [0.2710,  0.0192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6971,  0.2710], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin1(torch.tensor([1.,0.]))\n",
    "# lin1(torch.tensor([0.,1.]))\n",
    "# lin1(torch.tensor([1.,1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Linear Transformation and an activation function is shown in the animation above. Three things happen here: Stretching, Sliding and Squification. The first two are performed by the linear transformation and the last is performed by a activation function.  So, when vectors are passed as input they are mapped to a different space. \n",
    "\n",
    "Now, I suspect many of you might be full of questions and an exhaustive analysis of Linear Transformation is beyond the scope of this lecture. But, [here's](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) an excellect series by the wonderful [3Blue1Brown](https://twitter.com/3Blue1Brown) on Linear Transformations which without a doubt will cover most of your questions. Seriously folks, it changed how I look at Deep Learning and without a doubt it will change your prespective as well. Also, I have taken the above animation from [Chis Olah](https://twitter.com/ch402)'s wonderful [blogpost](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) about NNs and topology (which I would suggest you only look at after the LA course it might confuse you further)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [.1,.1]\n",
    "\n",
    "# # lin1 = nn.Linear(2,2, bias=False)\n",
    "# # lin2 = nn.Linear(2,2, bias=False)\n",
    "\n",
    "# b = lin1(torch.tensor(a))\n",
    "# c = lin2(torch.tensor(a))\n",
    "\n",
    "# print(b)\n",
    "# print(c)\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# V = np.array([[.1,.1], [-.4452,.6256], [-.0827,.1850]])\n",
    "# origin = np.array([[0, 0, 0],[0, 0, 0]]) # origin point\n",
    "\n",
    "# plt.quiver(*origin, V[:,0], V[:,1], color=['r','b','g'], scale=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is SoftMax? Not Perfect  ## \n",
    "Given a set of scores the Softmax func is used to output probabilities. The most common use case is that of classification. The setup in which softmax is commonly used is something like this: \n",
    "\n",
    "You have a network of some sort which outputs a vector of a given size and you want to use this vector to perform classification. So, if you have 5 labels you use a Linear Layer to use this vector to create 5 scores. \n",
    "For example: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,100) # Fixed Length Vector Representing you input\n",
    "lin = nn.Linear(100,5)\n",
    "y = lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3326, -0.9005, -1.0053,  0.1555,  0.1022]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-776-a481467c755f>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1904, 0.1079, 0.0972, 0.3103, 0.2942]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have used the Softmax to convert the scores into probabilities. Now, what does the softmax do which makes me say that these are probabilities? That is related to how the softmax computes these scores. So, all that softmax does is that it normalizes the scores given to it and one of the properties of the scores generated by SoftMax is that they will always sum to 1. \n",
    "\n",
    "So, in our example, if you know that the input definitely belongs to one of the 5 classes and only one of the 5 classes softmax will normalize the scores so that the probabilities sum up to 1.\n",
    "\n",
    "So, to conclude, What does Softmax do? \n",
    "Given a set of scores it will normalize the scores so that they sum to 1 and this allows us to think of the output as probabilities. \n",
    "\n",
    "Now, there are a few caveats. if your input could belong to multiple classes or none of the classes the probabilities of the softmax function can be misleading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
