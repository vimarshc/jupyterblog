---

title: The Attention Mechanism   


keywords: fastai
sidebar: home_sidebar



nb_path: "Attention I .ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: Attention I .ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">Introduction<a class="anchor-link" href="#Introduction"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/Caption_Generator_Attention.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://arxiv.org/pdf/1502.03044.pdf">Show, Attend and Tell: Neural Image CaptionGeneration with Visual Attention</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/Align_And_Translate_Attention.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://arxiv.org/pdf/1409.0473v7.pdf">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="I've-seen-what-attention-does-visually;-let's-take-a-step-forward">I've seen what attention does visually; let's take a step forward<a class="anchor-link" href="#I've-seen-what-attention-does-visually;-let's-take-a-step-forward"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/Align_And_Translate_Attention.png" alt=""></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">HDDN_DIM</span><span class="p">)</span> <span class="c1"># hidden = entity_hidden</span>

<span class="c1"># List of Entities</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ENC_DIM</span><span class="p">)</span> <span class="c1"># encoder_outputs = [entity_1, entity_2, entity_3]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([14, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span><span class="p">,</span> <span class="n">energy</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span><span class="n">encoder_outputs</span><span class="p">)</span>

<span class="c1"># bmm </span>
<span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
    <span class="n">encoder_outputs_attention_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">context_vector</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.0888, 0.1068, 0.0965, 0.1023, 0.0958, 0.0908, 0.0984, 0.1129, 0.1127,
         0.0950]], grad_fn=&lt;SoftmaxBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(1., grad_fn=&lt;SumBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Input-">The Input <a class="anchor-link" href="#The-Input-"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([14, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Output-">The Output <a class="anchor-link" href="#The-Output-"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span><span class="p">,</span> <span class="n">energy</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([[0.0897, 0.0583, 0.0661, 0.0786, 0.0655, 0.0809, 0.0639, 0.0688, 0.0652,
          0.0619, 0.0821, 0.0815, 0.0630, 0.0745]], grad_fn=&lt;SoftmaxBackward&gt;),
 tensor([[ 0.1802, -0.2512, -0.1253,  0.0478, -0.1342,  0.0772, -0.1589, -0.0844,
          -0.1381, -0.1903,  0.0922,  0.0851, -0.1732, -0.0048]],
        grad_fn=&lt;SqueezeBackward1&gt;))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(1.0000, grad_fn=&lt;SumBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([50, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="BMM">BMM<a class="anchor-link" href="#BMM"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([140, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 140])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_small</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="c1"># [1.,2.]*0.7 = [0.7, 1.4]</span>
    <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span> <span class="c1"># [2.,1.]*0.2 = [0.4, 0.2]</span>
    <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]</span>  <span class="c1"># [1.,1.]*0.1 = [0.1, 0.1]</span>
<span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

<span class="n">attn_weights_small</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.7</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">]</span>
<span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_small</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([3, 1, 2])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn_weights_small</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 3])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
    <span class="n">attn_weights_small</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">encoder_outputs_small</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="c1"># This vector is supposed be to 70% of [1., 2.] and 20% of [2., 1.]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[1.2000, 1.7000]]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The context vector obtained is supposed to be a vector representation of:</p>
<ol>
<li>The image with the focus on the frisbee from the first example. </li>
<li>The source sentence with the focus on the word 'in' in the NMT example. </li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Internals-of-the-Attention-Mechanism">Internals of the Attention Mechanism<a class="anchor-link" href="#Internals-of-the-Attention-Mechanism"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># DEC_HID_DIM = 5</span>

<span class="n">HDDN_DIM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">ENC_DIM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">DEC_DIM</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HDDN_DIM</span> <span class="o">+</span> <span class="n">ENC_DIM</span><span class="p">,</span> <span class="n">DEC_DIM</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">DEC_DIM</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Linear(in_features=10, out_features=5, bias=True)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">HDDN_DIM</span><span class="p">)</span> <span class="c1"># hidden = entity_hidden</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ENC_DIM</span><span class="p">)</span> <span class="c1"># encoder_outputs = [entity_1, entity_2, entity_3]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span> <span class="c1">#(seq_len, batchsize, vector_size)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([10, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span><span class="n">encoder_outputs</span><span class="p">):</span>
<span class="c1">#     import pdb;pdb.set_trace()</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">src_len</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1">#repeat decoder hidden state src_len times</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># energy function </span>
    <span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
        <span class="n">attn</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">(</span><span class="n">hidden</span><span class="p">,</span>
                <span class="n">encoder_outputs</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span> 
    <span class="n">attention</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">energy</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">attention</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([10, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Walking-Back-">Walking Back <a class="anchor-link" href="#Walking-Back-"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span><span class="p">,</span> <span class="n">energy</span>  <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

<span class="c1"># bmm </span>
<span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
    <span class="n">encoder_outputs_attention_softmax_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The probability $\alpha_{ij}$ , or its associated energy $e_{ij}$, reflects the importance of the annotation $h_{j}$ with respect to the previous hidden states $iâˆ’1$ in deciding the next states $i$ and generating $y_i$ . Intuitively,this implements a mechanism of attention in the decoder.  The decoder decides parts of the source sentence to pay attention to.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">HDDN_DIM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">ENC_DIM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">DEC_DIM</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HDDN_DIM</span> <span class="o">+</span> <span class="n">ENC_DIM</span><span class="p">,</span> <span class="n">DEC_DIM</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">DEC_DIM</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">HDDN_DIM</span> <span class="o">+</span> <span class="n">ENC_DIM</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 10])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn</span><span class="p">(</span><span class="n">q</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">HDDN_DIM</span><span class="p">)</span> <span class="c1"># hidden = entity_hidden</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ENC_DIM</span><span class="p">)</span> <span class="c1"># encoder_outputs = [entity_1, entity_2, entity_3]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Attention-Function">The Attention Function<a class="anchor-link" href="#The-Attention-Function"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's zoom into the attention func. I've taken every line and put it in a different cell.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([14, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">src_len</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">src_len</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>14</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 14, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 14, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
    <span class="n">attn</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">(</span><span class="n">hidden</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 14, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
    <span class="n">attn</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">(</span><span class="n">hidden</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attention</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">energy</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">energy</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 14, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attention</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.3557, 0.3651, 0.4421, 0.4193, 0.4137, 0.6503, 0.3485, 0.2695, 0.4164,
         0.5820, 0.4092, 0.2467, 0.5506, 0.5263]], grad_fn=&lt;SqueezeBackward1&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(1., grad_fn=&lt;SumBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-Energy-Function">The Energy Function<a class="anchor-link" href="#The-Energy-Function"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/energy_functions.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://arxiv.org/pdf/1508.04025.pdf">Effective Approaches to Attention-based Neural Machine Translation</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="General-Attention">General Attention<a class="anchor-link" href="#General-Attention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_att_weight</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">):</span>  <span class="c1"># get attention weight one &#39;dec_output&#39; with &#39;enc_outputs&#39;</span>
    <span class="n">n_step</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">)</span>
    <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_step</span><span class="p">)</span>  <span class="c1"># attn_scores : [n_step]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_step</span><span class="p">):</span>
        <span class="n">attn_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_att_score</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="c1"># Normalize scores to weights in range 0 to 1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">attn_scores</span>

<span class="k">def</span> <span class="nf">get_att_score</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span>  <span class="c1"># enc_outputs [batch_size, num_directions(=1) * n_hidden]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>  <span class="c1"># score : [batch_size, n_hidden]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dec_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">score</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># inner product make scalar value</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span><span class="p">,</span> <span class="n">energy</span> <span class="o">=</span> <span class="n">get_att_weight</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-152-198907164ce3&gt;:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(attn_scores).view(1, 1, -1), attn_scores
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[3.4176e-04, 8.2757e-01, 1.7209e-01]]], grad_fn=&lt;ViewBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">energy</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([-5.8196,  1.9725,  0.4020], grad_fn=&lt;CopySlices&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Further-Reading">Further Reading<a class="anchor-link" href="#Further-Reading"> </a></h2><p>Before we move on to the next topic we've covered enough ground here for you to be able to look at implementations of the attention mechanisms and/or read research papers that utilize the attention mechanism for various tasks.</p>
<p><a href="https://arxiv.org/pdf/1409.0473v7.pdf"><strong>NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</strong></a>: This paper introduced the attention mechanism and it did that in the context of the NMT task. <a href="https://paperswithcode.com/paper/neural-machine-translation-by-jointly">Here</a> are the implementations of this paper on paperwithcode.</p>
<p><a href="https://arxiv.org/pdf/1508.04025.pdf"><strong>Effective Approaches to Attention-based Neural Machine Translation</strong></a>: This paper introduces a new energy function as I discussed above.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Self-Attention">Self-Attention<a class="anchor-link" href="#Self-Attention"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://huggingface.co/transformers/">transformers</a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.models.distilbert.modeling_distilbert</span> <span class="kn">import</span> <span class="n">MultiHeadSelfAttention</span>
<span class="kn">from</span> <span class="nn">transformers.models.distilbert.configuration_distilbert</span> <span class="kn">import</span> <span class="n">DistilBertConfig</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>PreTrainedTokenizerFast(name_or_path=&#39;distilbert-base-uncased&#39;, vocab_size=30522, model_max_len=512, is_fast=True, padding_side=&#39;right&#39;, special_tokens={&#39;unk_token&#39;: &#39;[UNK]&#39;, &#39;sep_token&#39;: &#39;[SEP]&#39;, &#39;pad_token&#39;: &#39;[PAD]&#39;, &#39;cls_token&#39;: &#39;[CLS]&#39;, &#39;mask_token&#39;: &#39;[MASK]&#39;})</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">embeddify</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
    <span class="n">_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">multi_head_attn</span> <span class="o">=</span> <span class="n">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">config</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>DistilBertConfig {
  &#34;activation&#34;: &#34;gelu&#34;,
  &#34;attention_dropout&#34;: 0.1,
  &#34;dim&#34;: 768,
  &#34;dropout&#34;: 0.1,
  &#34;hidden_dim&#34;: 3072,
  &#34;initializer_range&#34;: 0.02,
  &#34;max_position_embeddings&#34;: 512,
  &#34;model_type&#34;: &#34;distilbert&#34;,
  &#34;n_heads&#34;: 12,
  &#34;n_layers&#34;: 6,
  &#34;pad_token_id&#34;: 0,
  &#34;qa_dropout&#34;: 0.1,
  &#34;seq_classif_dropout&#34;: 0.2,
  &#34;sinusoidal_pos_embds&#34;: false,
  &#34;vocab_size&#34;: 30522
}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embeddified_text</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">embeddify</span><span class="p">(</span><span class="s1">&#39;Ronaldo is one of the best football players in the world&#39;</span><span class="p">)</span>
<span class="c1"># x = torch.randn(1,10,config.dim) # (bs, seq_length, dim)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-795-4b542d131dd7&gt;:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return embeddings(torch.tensor(torch.tensor(tokenizer(text)[&#34;input_ids&#34;]).view(1,_len))), tokenizer.convert_ids_to_tokens(token_ids)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embeddified_text</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 14, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;[CLS]&#39;,
 &#39;ronald&#39;,
 &#39;##o&#39;,
 &#39;is&#39;,
 &#39;one&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;best&#39;,
 &#39;football&#39;,
 &#39;players&#39;,
 &#39;in&#39;,
 &#39;the&#39;,
 &#39;world&#39;,
 &#39;[SEP]&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">14</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">multi_head_attn_op</span> <span class="o">=</span> <span class="n">multi_head_attn</span><span class="p">(</span>
    <span class="n">embeddified_text</span><span class="p">,</span>
    <span class="n">embeddified_text</span><span class="p">,</span>
    <span class="n">embeddified_text</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">multi_head_attn_op</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>1</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">multi_head_attn_op</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 14, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Self-Attention:-Definition">Self Attention: Definition<a class="anchor-link" href="#Self-Attention:-Definition"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span><span class="p">,</span> <span class="n">energy</span>  <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

<span class="c1"># bmm </span>
<span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
    <span class="n">encoder_outputs_attention_softmax_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/transformer_self-attention_visualization.png" alt=""></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">chatuur_multi_head_attn</span> <span class="o">=</span> <span class="n">Chatuur_MultiHeadSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">op</span> <span class="o">=</span> <span class="n">chatuur_multi_head_attn</span><span class="p">(</span><span class="n">embeddified_text</span><span class="p">,</span>
                             <span class="n">embeddified_text</span><span class="p">,</span>
                             <span class="n">embeddified_text</span><span class="p">,</span>
                             <span class="n">mask</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(46)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     44 </span><span class="ansi-red-fg">        &#34;&#34;&#34;
</span><span class="ansi-green-fg">     45 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">import</span> pdb<span class="ansi-blue-fg">;</span>pdb<span class="ansi-blue-fg">.</span>set_trace<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 46 </span><span class="ansi-red-fg">        </span>bs<span class="ansi-blue-fg">,</span> q_length<span class="ansi-blue-fg">,</span> dim <span class="ansi-blue-fg">=</span> query<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     47 </span><span class="ansi-red-fg">        </span>k_length <span class="ansi-blue-fg">=</span> key<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     48 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert dim == self.dim, &#39;Dimensions do not match: %s input vs %s configured&#39; % (dim, self.dim)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(47)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     45 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">import</span> pdb<span class="ansi-blue-fg">;</span>pdb<span class="ansi-blue-fg">.</span>set_trace<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     46 </span><span class="ansi-red-fg">        </span>bs<span class="ansi-blue-fg">,</span> q_length<span class="ansi-blue-fg">,</span> dim <span class="ansi-blue-fg">=</span> query<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 47 </span><span class="ansi-red-fg">        </span>k_length <span class="ansi-blue-fg">=</span> key<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     48 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert dim == self.dim, &#39;Dimensions do not match: %s input vs %s configured&#39; % (dim, self.dim)</span>
<span class="ansi-green-fg">     49 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert key.size() == value.size()</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(51)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     49 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert key.size() == value.size()</span>
<span class="ansi-green-fg">     50 </span>
<span class="ansi-green-fg">---&gt; 51 </span><span class="ansi-red-fg">        </span>dim_per_head <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dim <span class="ansi-blue-fg">//</span> self<span class="ansi-blue-fg">.</span>n_heads
<span class="ansi-green-fg">     52 </span>
<span class="ansi-green-fg">     53 </span><span class="ansi-red-fg">        </span>mask_reshp <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> k_length<span class="ansi-blue-fg">)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(53)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     51 </span><span class="ansi-red-fg">        </span>dim_per_head <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dim <span class="ansi-blue-fg">//</span> self<span class="ansi-blue-fg">.</span>n_heads
<span class="ansi-green-fg">     52 </span>
<span class="ansi-green-fg">---&gt; 53 </span><span class="ansi-red-fg">        </span>mask_reshp <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> k_length<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     54 </span>
<span class="ansi-green-fg">     55 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">def</span> shape<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(55)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     53 </span><span class="ansi-red-fg">        </span>mask_reshp <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> k_length<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     54 </span>
<span class="ansi-green-fg">---&gt; 55 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">def</span> shape<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">     56 </span><span class="ansi-red-fg">            </span><span class="ansi-blue-fg">&#34;&#34;&#34; separate heads &#34;&#34;&#34;</span>
<span class="ansi-green-fg">     57 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads<span class="ansi-blue-fg">,</span> dim_per_head<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(59)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     57 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads<span class="ansi-blue-fg">,</span> dim_per_head<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     58 </span>
<span class="ansi-green-fg">---&gt; 59 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">def</span> unshape<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">     60 </span><span class="ansi-red-fg">            </span><span class="ansi-blue-fg">&#34;&#34;&#34; group heads &#34;&#34;&#34;</span>
<span class="ansi-green-fg">     61 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>contiguous<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads <span class="ansi-blue-fg">*</span> dim_per_head<span class="ansi-blue-fg">)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(63)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     61 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>contiguous<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads <span class="ansi-blue-fg">*</span> dim_per_head<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     62 </span>
<span class="ansi-green-fg">---&gt; 63 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>q_lin<span class="ansi-blue-fg">(</span>query<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     64 </span><span class="ansi-red-fg">        </span>k <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>k_lin<span class="ansi-blue-fg">(</span>key<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(64)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     62 </span>
<span class="ansi-green-fg">     63 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>q_lin<span class="ansi-blue-fg">(</span>query<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 64 </span><span class="ansi-red-fg">        </span>k <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>k_lin<span class="ansi-blue-fg">(</span>key<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     66 </span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(65)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     63 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>q_lin<span class="ansi-blue-fg">(</span>query<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     64 </span><span class="ansi-red-fg">        </span>k <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>k_lin<span class="ansi-blue-fg">(</span>key<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     66 </span>
<span class="ansi-green-fg">     67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>

ipdb&gt; 
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(67)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     66 </span>
<span class="ansi-green-fg">---&gt; 67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>

ipdb&gt; p q.shape
torch.Size([1, 12, 14, 64])
ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(68)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     66 </span>
<span class="ansi-green-fg">     67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     70 </span><span class="ansi-red-fg">        </span>scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>

ipdb&gt; p k.transpose(2, 3).shape
torch.Size([1, 12, 64, 14])
ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(69)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">---&gt; 69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     70 </span><span class="ansi-red-fg">        </span>scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     71 </span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(70)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">---&gt; 70 </span><span class="ansi-red-fg">        </span>scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     71 </span>
<span class="ansi-green-fg">     72 </span><span class="ansi-red-fg">        </span>weights <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>Softmax<span class="ansi-blue-fg">(</span>dim<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>

ipdb&gt; p scores.shape
torch.Size([1, 12, 14, 14])
ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(72)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     70 </span><span class="ansi-red-fg">        </span>scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     71 </span>
<span class="ansi-green-fg">---&gt; 72 </span><span class="ansi-red-fg">        </span>weights <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>Softmax<span class="ansi-blue-fg">(</span>dim<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     73 </span><span class="ansi-red-fg">        </span>weights <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dropout<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     74 </span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(73)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     71 </span>
<span class="ansi-green-fg">     72 </span><span class="ansi-red-fg">        </span>weights <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>Softmax<span class="ansi-blue-fg">(</span>dim<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">---&gt; 73 </span><span class="ansi-red-fg">        </span>weights <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dropout<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     74 </span>
<span class="ansi-green-fg">     75 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># Mask heads if we want to</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(76)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     74 </span>
<span class="ansi-green-fg">     75 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># Mask heads if we want to</span>
<span class="ansi-green-fg">---&gt; 76 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">if</span> head_mask <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">     77 </span><span class="ansi-red-fg">            </span>weights <span class="ansi-blue-fg">=</span> weights <span class="ansi-blue-fg">*</span> head_mask
<span class="ansi-green-fg">     78 </span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(79)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     77 </span><span class="ansi-red-fg">            </span>weights <span class="ansi-blue-fg">=</span> weights <span class="ansi-blue-fg">*</span> head_mask
<span class="ansi-green-fg">     78 </span>
<span class="ansi-green-fg">---&gt; 79 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">,</span> v<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     80 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> unshape<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-fg">     81 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>out_lin<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(80)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     78 </span>
<span class="ansi-green-fg">     79 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">,</span> v<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 80 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> unshape<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-fg">     81 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>out_lin<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-fg">     82 </span>

ipdb&gt; p context.shape
torch.Size([1, 12, 14, 64])
ipdb&gt; p weights.shape
torch.Size([1, 12, 14, 14])
ipdb&gt; v.shape
torch.Size([1, 12, 14, 64])
ipdb&gt; q
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">BdbQuit</span>                                   Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-821-40fe8d92a2d0&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> op = chatuur_multi_head_attn(embeddified_text,
</span><span class="ansi-green-intense-fg ansi-bold">      2</span>                              embeddified_text<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>                              embeddified_text<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>                              mask)

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    725</span>             result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_slow_forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    726</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 727</span><span class="ansi-red-fg">             </span>result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    728</span>         for hook in itertools.chain(
<span class="ansi-green-intense-fg ansi-bold">    729</span>                 _global_forward_hooks<span class="ansi-blue-fg">.</span>values<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, query, key, value, mask, head_mask, output_attentions)</span>
<span class="ansi-green-intense-fg ansi-bold">     78</span> 
<span class="ansi-green-intense-fg ansi-bold">     79</span>         context <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">,</span> v<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 80</span><span class="ansi-red-fg">         </span>context <span class="ansi-blue-fg">=</span> unshape<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-intense-fg ansi-bold">     81</span>         context <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>out_lin<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-intense-fg ansi-bold">     82</span> 

<span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, query, key, value, mask, head_mask, output_attentions)</span>
<span class="ansi-green-intense-fg ansi-bold">     78</span> 
<span class="ansi-green-intense-fg ansi-bold">     79</span>         context <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">,</span> v<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 80</span><span class="ansi-red-fg">         </span>context <span class="ansi-blue-fg">=</span> unshape<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-intense-fg ansi-bold">     81</span>         context <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>out_lin<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-intense-fg ansi-bold">     82</span> 

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/bdb.py</span> in <span class="ansi-cyan-fg">trace_dispatch</span><span class="ansi-blue-fg">(self, frame, event, arg)</span>
<span class="ansi-green-intense-fg ansi-bold">     86</span>             <span class="ansi-green-fg">return</span> <span class="ansi-red-fg"># None</span>
<span class="ansi-green-intense-fg ansi-bold">     87</span>         <span class="ansi-green-fg">if</span> event <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;line&#39;</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 88</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>dispatch_line<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     89</span>         <span class="ansi-green-fg">if</span> event <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;call&#39;</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     90</span>             <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>dispatch_call<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">,</span> arg<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/bdb.py</span> in <span class="ansi-cyan-fg">dispatch_line</span><span class="ansi-blue-fg">(self, frame)</span>
<span class="ansi-green-intense-fg ansi-bold">    111</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>stop_here<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">or</span> self<span class="ansi-blue-fg">.</span>break_here<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    112</span>             self<span class="ansi-blue-fg">.</span>user_line<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 113</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>quitting<span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">raise</span> BdbQuit
<span class="ansi-green-intense-fg ansi-bold">    114</span>         <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>trace_dispatch
<span class="ansi-green-intense-fg ansi-bold">    115</span> 

<span class="ansi-red-fg">BdbQuit</span>: </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Chatuur_MultiHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">)</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">q_lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads</span><span class="p">):</span>
        <span class="n">attention_head_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">heads</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">find_pruneable_heads_and_indices</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">attention_head_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">)</span>
        <span class="c1"># Prune linear layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_lin</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lin</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_lin</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_lin</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_lin</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_lin</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_lin</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_lin</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Update hyper params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">attention_head_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters:</span>
<span class="sd">            query: torch.tensor(bs, seq_length, dim)</span>
<span class="sd">            key: torch.tensor(bs, seq_length, dim)</span>
<span class="sd">            value: torch.tensor(bs, seq_length, dim)</span>
<span class="sd">            mask: torch.tensor(bs, seq_length)</span>

<span class="sd">        Returns:</span>
<span class="sd">            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,</span>
<span class="sd">            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">pdb</span><span class="p">;</span><span class="n">pdb</span><span class="o">.</span><span class="n">set_trace</span><span class="p">()</span>
        <span class="n">bs</span><span class="p">,</span> <span class="n">q_length</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">k_length</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># assert dim == self.dim, &#39;Dimensions do not match: %s input vs %s configured&#39; % (dim, self.dim)</span>
        <span class="c1"># assert key.size() == value.size()</span>

        <span class="n">dim_per_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>

        <span class="n">mask_reshp</span> <span class="o">=</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k_length</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot; separate heads &quot;&quot;&quot;</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">dim_per_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">unshape</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot; group heads &quot;&quot;&quot;</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="n">dim_per_head</span><span class="p">)</span>

        <span class="c1"># query object </span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lin</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>  <span class="c1"># (bs, n_heads, q_length, dim_per_head)</span>
        
        <span class="c1"># list of things</span>
        <span class="c1"># Discuss the rearrangement for multi heads. </span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_lin</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>  <span class="c1"># (bs, n_heads, k_length, dim_per_head)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_lin</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>  <span class="c1"># (bs, n_heads, k_length, dim_per_head)</span>

        <span class="c1"># Attention All you Need paper states tha this operation improves results. </span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dim_per_head</span><span class="p">)</span>  <span class="c1"># (bs, n_heads, q_length, dim_per_head)</span>
        
        
        <span class="c1"># Dot Energy function. </span>
        <span class="c1"># show we have a score for each word being treated as query </span>
        <span class="c1"># and performing attention on itself. </span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># (bs, n_heads, q_length, k_length)</span>
        

        
        <span class="c1"># Will talk about this later</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">mask_reshp</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>  <span class="c1"># (bs, n_heads, q_length, k_length)</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>  <span class="c1"># (bs, n_heads, q_length, k_length)</span>

        <span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">scores</span><span class="p">)</span>  <span class="c1"># (bs, n_heads, q_length, k_length)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>  <span class="c1"># (bs, n_heads, q_length, k_length)</span>

        <span class="c1"># Mask heads if we want to</span>
        <span class="k">if</span> <span class="n">head_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">*</span> <span class="n">head_mask</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># (bs, n_heads, q_length, dim_per_head)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">unshape</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>  <span class="c1"># (bs, q_length, dim)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_lin</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>  <span class="c1"># (bs, q_length, dim)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">context</span><span class="p">,)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, I am leaving a few questions for the next video:</p>
<ol>
<li>The purpose of q_lin, k_lin and v_lin. </li>
<li>What is a mask. </li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="When-I'm-using-Attention-What-does-that-look-like?">When I'm using Attention What does that look like?<a class="anchor-link" href="#When-I'm-using-Attention-What-does-that-look-like?"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#     1. Fetches information from a list of entities. </span>
<span class="c1">#     2. Represents the fetched information as one entity</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">HDDN_DIM</span><span class="p">)</span> <span class="c1"># hidden = entity_hidden</span>

<span class="c1"># List of Entities</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ENC_DIM</span><span class="p">)</span> <span class="c1"># encoder_outputs = [entity_1, entity_2, entity_3]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([5, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span><span class="p">,</span> <span class="n">energy</span>  <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

<span class="c1"># bmm </span>
<span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
    <span class="n">encoder_outputs_attention_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.1984, 0.2289, 0.1863, 0.1938, 0.1925]], grad_fn=&lt;SoftmaxBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">context_vector</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.3430, 0.3196, 0.3373]], grad_fn=&lt;SoftmaxBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([3, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">100.</span><span class="p">,</span> <span class="mf">75.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]]),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[1.0000e+00, 1.3888e-11, 5.5211e-42, 2.0305e-42]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">0.37</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.0524,  0.0319, -0.1579, -0.6566,  0.0464]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mf">0.33</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.3120,  0.5839, -0.3179, -0.1141, -0.1452]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mf">0.29</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.2742,  0.5132, -0.2793, -0.1002, -0.1276]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">0.3704</span> <span class="o">+</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mf">0.3327</span> <span class="o">+</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="mf">0.2969</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.3082,  1.0963, -0.7871, -0.5865, -0.1027]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
    <span class="n">encoder_outputs_attention_softmax_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">context_vector</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[ 0.3082,  1.0963, -0.7871, -0.5866, -0.1027]]],
       grad_fn=&lt;BmmBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">context_vector</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[ 1.0724, -0.2875,  0.0412, -0.3702,  0.0347]]],
       grad_fn=&lt;BmmBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">energy</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.3601, 0.4211, 0.5020]], grad_fn=&lt;SqueezeBackward1&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.3110, 0.3306, 0.3584]], grad_fn=&lt;SoftmaxBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.9320, -2.8623,  0.2094,  0.2498, -0.2511]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[-2.6336e-05,  8.2289e-01,  7.7262e-02,  1.9560e+00,  1.5071e-02]],

        [[ 7.4831e-02, -1.6805e+00, -9.8090e-01, -6.8144e-01, -7.4640e-03]],

        [[-1.5339e-01, -6.5884e-03,  3.0120e-01, -1.7828e+00, -1.1082e+00]]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encoder_outputs_attention_softmax_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoder_outputs_attention_softmax_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>torch.Size([1, 1, 3])
torch.Size([1, 3])
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>torch.Size([1, 3, 5])
torch.Size([3, 1, 5])
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
    <span class="n">encoder_outputs_attention_softmax_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.3234, 0.3342, 0.3424]], grad_fn=&lt;SoftmaxBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[-2.6336e-05,  8.2289e-01,  7.7262e-02,  1.9560e+00,  1.5071e-02]],

        [[ 7.4831e-02, -1.6805e+00, -9.8090e-01, -6.8144e-01, -7.4640e-03]],

        [[-1.5339e-01, -6.5884e-03,  3.0120e-01, -1.7828e+00, -1.1082e+00]]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="mf">0.2442</span><span class="o">*</span><span class="mf">0.3492</span> <span class="o">+</span> <span class="o">-</span><span class="mf">1.0587</span><span class="o">*</span><span class="mf">0.2922</span> <span class="o">+</span> <span class="mf">0.5481</span><span class="o">*</span><span class="mf">0.3586</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>-0.02752884</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">context_vector</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[-0.0275, -0.2978, -0.1997, -0.2056, -0.3770]]],
       grad_fn=&lt;BmmBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-Energy-Function">The Energy Function<a class="anchor-link" href="#The-Energy-Function"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/energy_functions.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="General-Attention">General Attention<a class="anchor-link" href="#General-Attention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_att_weight</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">):</span>  <span class="c1"># get attention weight one &#39;dec_output&#39; with &#39;enc_outputs&#39;</span>
    <span class="n">n_step</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">)</span>
    <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_step</span><span class="p">)</span>  <span class="c1"># attn_scores : [n_step]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_step</span><span class="p">):</span>
        <span class="n">attn_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_att_score</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="c1"># Normalize scores to weights in range 0 to 1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">attn_scores</span>

<span class="k">def</span> <span class="nf">get_att_score</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span>  <span class="c1"># enc_outputs [batch_size, num_directions(=1) * n_hidden]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>  <span class="c1"># score : [batch_size, n_hidden]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dec_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">score</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># inner product make scalar value</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span><span class="p">,</span> <span class="n">energy</span> <span class="o">=</span> <span class="n">get_att_weight</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-152-198907164ce3&gt;:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(attn_scores).view(1, 1, -1), attn_scores
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[3.4176e-04, 8.2757e-01, 1.7209e-01]]], grad_fn=&lt;ViewBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">energy</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([-5.8196,  1.9725,  0.4020], grad_fn=&lt;CopySlices&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-context-of-Discovery:-NMT">The context of Discovery: NMT<a class="anchor-link" href="#The-context-of-Discovery:-NMT"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/seq2seq1.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate"><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a><a class="anchor-link" href="#Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1-Introduction">1 Introduction<a class="anchor-link" href="#1-Introduction"> </a></h2><p>Neural machine translationis a newly emerging approach to machine translation, recently proposed by Kalchbrenner and Blunsom (2013), Sutskeveret al.(2014) and Choet al.(2014b).  Unlike thetraditional phrase-based translation system (see, e.g., Koehnet al., 2003) which consists of manysmall sub-components that are tuned separately, neural machine translation attempts to build andtrain a single, large neural network that reads a sentence and outputs a correct translation.</p>
<p>Most  of  the  proposed  neural  machine  translation  models  belong  to  a  family  of encoderâ€“decoders(Sutskeveret al., 2014; Choet al., 2014a), with an encoder and a decoder for each lan-guage, or involve a language-specific encoder applied to each sentence whose outputs are then com-pared (Hermann and Blunsom, 2014).  An encoder neural network reads and encodes a source sen-tence into a fixed-length vector. A decoder then outputs a translation from the encoded vector. Thewhole encoderâ€“decoder system, which consists of the encoder and the decoder for a language pair,is jointly trained to maximize the probability of a correct translation given a source sentence</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rnn_example_seq_tensor</span><span class="p">,</span> <span class="n">seq</span> <span class="o">=</span> <span class="n">embeddify</span><span class="p">(</span><span class="s1">&#39;This person is a good person&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-222-4b542d131dd7&gt;:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return embeddings(torch.tensor(torch.tensor(tokenizer(text)[&#34;input_ids&#34;]).view(1,_len))), tokenizer.convert_ids_to_tokens(token_ids)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rnn_example_seq_tensor</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 6, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">seq</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;This&#39;, &#39;person&#39;, &#39;is&#39;, &#39;a&#39;, &#39;good&#39;, &#39;person&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rnn_example_hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">768</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">example_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span><span class="mi">768</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="p">,</span><span class="n">last_encoder_output</span> <span class="o">=</span> <span class="n">example_rnn</span><span class="p">(</span><span class="n">rnn_example_seq_tensor</span><span class="p">,</span><span class="n">rnn_example_hidden</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 8, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">last_encoder_output</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 1, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">last_encoder_output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># S: Symbol that shows starting of decoding input</span>
<span class="c1"># E: Symbol that shows starting of decoding output</span>
<span class="c1"># P: Symbol that will fill in blank sequence if current batch data size is short than time steps</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Model</span>
<span class="k">class</span> <span class="nc">Seq2Seq</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Seq2Seq</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">enc_cell</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">n_class</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_cell</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">n_class</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_class</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_input</span><span class="p">,</span> <span class="n">enc_hidden</span><span class="p">,</span> <span class="n">dec_input</span><span class="p">):</span>
        <span class="c1"># batch_size, seq_len, vector_len</span>
        
        <span class="kn">import</span> <span class="nn">pdb</span><span class="p">;</span><span class="n">pdb</span><span class="o">.</span><span class="n">set_trace</span><span class="p">()</span>
        <span class="n">enc_input</span> <span class="o">=</span> <span class="n">enc_input</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># enc_input: [max_len(=n_step, time step), batch_size, n_class]</span>
        <span class="n">dec_input</span> <span class="o">=</span> <span class="n">dec_input</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># dec_input: [max_len(=n_step, time step), batch_size, n_class]</span>

        <span class="c1"># enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">enc_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_cell</span><span class="p">(</span><span class="n">enc_input</span><span class="p">,</span> <span class="n">enc_hidden</span><span class="p">)</span>
        <span class="c1"># outputs : [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)]</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_cell</span><span class="p">(</span><span class="n">dec_input</span><span class="p">,</span> <span class="n">enc_states</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="c1"># model : [max_len+1(=6), batch_size, n_class]</span>
        <span class="k">return</span> <span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">make_batch</span><span class="p">():</span>
    <span class="n">input_batch</span><span class="p">,</span> <span class="n">output_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">seq_data</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="n">seq</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">seq</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;P&#39;</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_step</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

        <span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_dic</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_dic</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;S&#39;</span> <span class="o">+</span> <span class="n">seq</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_dic</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">(</span><span class="n">seq</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;E&#39;</span><span class="p">)]</span>
        
        <span class="n">input_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_class</span><span class="p">)[</span><span class="nb">input</span><span class="p">])</span>
        <span class="n">output_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_class</span><span class="p">)[</span><span class="n">output</span><span class="p">])</span>
        <span class="n">target_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="c1"># not one-hot</span>

    <span class="c1"># make tensor</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">input_batch</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">output_batch</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target_batch</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_step</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">char_arr</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="s1">&#39;SEPabcdefghijklmnopqrstuvwxyz&#39;</span><span class="p">]</span>
<span class="n">num_dic</span> <span class="o">=</span> <span class="p">{</span><span class="n">n</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">char_arr</span><span class="p">)}</span>
<span class="n">seq_data</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;women&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="s1">&#39;white&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;queen&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;girl&#39;</span><span class="p">,</span> <span class="s1">&#39;boy&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;up&#39;</span><span class="p">,</span> <span class="s1">&#39;down&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;high&#39;</span><span class="p">,</span> <span class="s1">&#39;low&#39;</span><span class="p">]]</span>

<span class="n">n_class</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_dic</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_data</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Seq2Seq</span><span class="p">()</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">input_batch</span><span class="p">,</span> <span class="n">output_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="o">=</span> <span class="n">make_batch</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/vimarshc/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  warnings.warn(&#34;dropout option adds dropout after all but last &#34;
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">seq_data</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>6</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">input_batch</span><span class="o">.</span><span class="n">shape</span>
<span class="c1"># batch_size, seq_len, vector_len</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 5, 29])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># make hidden shape [num_layers * num_directions, batch_size, n_hidden]</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">output_batch</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># [batch_size, max_len+1(=6), n_class]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_batch</span><span class="p">)):</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">target_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch:&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%04d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;cost =&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{:.6f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Test</span>
<span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="n">input_batch</span><span class="p">,</span> <span class="n">output_batch</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_batch</span><span class="p">([[</span><span class="n">word</span><span class="p">,</span> <span class="s1">&#39;P&#39;</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)]])</span>

    <span class="c1"># make hidden shape [num_layers * num_directions, batch_size, n_hidden]</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">output_batch</span><span class="p">)</span>
    <span class="c1"># output : [max_len+1(=6), batch_size(=1), n_class]</span>

    <span class="n">predict</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># select n_class dimension</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">char_arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">predict</span><span class="p">]</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">decoded</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;E&#39;</span><span class="p">)</span>
    <span class="n">translated</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">decoded</span><span class="p">[:</span><span class="n">end</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">translated</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;P&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&gt; <span class="ansi-green-fg">&lt;ipython-input-188-82550f9d53b1&gt;</span>(12)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     10 </span><span class="ansi-red-fg">    </span><span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> enc_input<span class="ansi-blue-fg">,</span> enc_hidden<span class="ansi-blue-fg">,</span> dec_input<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">     11 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">import</span> pdb<span class="ansi-blue-fg">;</span>pdb<span class="ansi-blue-fg">.</span>set_trace<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 12 </span><span class="ansi-red-fg">        </span>enc_input <span class="ansi-blue-fg">=</span> enc_input<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># enc_input: [max_len(=n_step, time step), batch_size, n_class]</span>
<span class="ansi-green-fg">     13 </span><span class="ansi-red-fg">        </span>dec_input <span class="ansi-blue-fg">=</span> dec_input<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># dec_input: [max_len(=n_step, time step), batch_size, n_class]</span>
<span class="ansi-green-fg">     14 </span>

ipdb&gt; p enc_input.shape
torch.Size([6, 5, 29])
ipdb&gt; q
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">BdbQuit</span>                                   Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-193-5cbb71501101&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> 
<span class="ansi-green-intense-fg ansi-bold">      5</span>     optimizer<span class="ansi-blue-fg">.</span>zero_grad<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">----&gt; 6</span><span class="ansi-red-fg">     </span>output <span class="ansi-blue-fg">=</span> model<span class="ansi-blue-fg">(</span>input_batch<span class="ansi-blue-fg">,</span> hidden<span class="ansi-blue-fg">,</span> output_batch<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      7</span> 
<span class="ansi-green-intense-fg ansi-bold">      8</span>     output <span class="ansi-blue-fg">=</span> output<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># [batch_size, max_len+1(=6), n_class]</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    725</span>             result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_slow_forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    726</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 727</span><span class="ansi-red-fg">             </span>result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    728</span>         for hook in itertools.chain(
<span class="ansi-green-intense-fg ansi-bold">    729</span>                 _global_forward_hooks<span class="ansi-blue-fg">.</span>values<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">&lt;ipython-input-188-82550f9d53b1&gt;</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, enc_input, enc_hidden, dec_input)</span>
<span class="ansi-green-intense-fg ansi-bold">     10</span>     <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> enc_input<span class="ansi-blue-fg">,</span> enc_hidden<span class="ansi-blue-fg">,</span> dec_input<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     11</span>         <span class="ansi-green-fg">import</span> pdb<span class="ansi-blue-fg">;</span>pdb<span class="ansi-blue-fg">.</span>set_trace<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 12</span><span class="ansi-red-fg">         </span>enc_input <span class="ansi-blue-fg">=</span> enc_input<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># enc_input: [max_len(=n_step, time step), batch_size, n_class]</span>
<span class="ansi-green-intense-fg ansi-bold">     13</span>         dec_input <span class="ansi-blue-fg">=</span> dec_input<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># dec_input: [max_len(=n_step, time step), batch_size, n_class]</span>
<span class="ansi-green-intense-fg ansi-bold">     14</span> 

<span class="ansi-green-fg">&lt;ipython-input-188-82550f9d53b1&gt;</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, enc_input, enc_hidden, dec_input)</span>
<span class="ansi-green-intense-fg ansi-bold">     10</span>     <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> enc_input<span class="ansi-blue-fg">,</span> enc_hidden<span class="ansi-blue-fg">,</span> dec_input<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     11</span>         <span class="ansi-green-fg">import</span> pdb<span class="ansi-blue-fg">;</span>pdb<span class="ansi-blue-fg">.</span>set_trace<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 12</span><span class="ansi-red-fg">         </span>enc_input <span class="ansi-blue-fg">=</span> enc_input<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># enc_input: [max_len(=n_step, time step), batch_size, n_class]</span>
<span class="ansi-green-intense-fg ansi-bold">     13</span>         dec_input <span class="ansi-blue-fg">=</span> dec_input<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># dec_input: [max_len(=n_step, time step), batch_size, n_class]</span>
<span class="ansi-green-intense-fg ansi-bold">     14</span> 

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/bdb.py</span> in <span class="ansi-cyan-fg">trace_dispatch</span><span class="ansi-blue-fg">(self, frame, event, arg)</span>
<span class="ansi-green-intense-fg ansi-bold">     86</span>             <span class="ansi-green-fg">return</span> <span class="ansi-red-fg"># None</span>
<span class="ansi-green-intense-fg ansi-bold">     87</span>         <span class="ansi-green-fg">if</span> event <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;line&#39;</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 88</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>dispatch_line<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     89</span>         <span class="ansi-green-fg">if</span> event <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;call&#39;</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     90</span>             <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>dispatch_call<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">,</span> arg<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/bdb.py</span> in <span class="ansi-cyan-fg">dispatch_line</span><span class="ansi-blue-fg">(self, frame)</span>
<span class="ansi-green-intense-fg ansi-bold">    111</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>stop_here<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">or</span> self<span class="ansi-blue-fg">.</span>break_here<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    112</span>             self<span class="ansi-blue-fg">.</span>user_line<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 113</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>quitting<span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">raise</span> BdbQuit
<span class="ansi-green-intense-fg ansi-bold">    114</span>         <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>trace_dispatch
<span class="ansi-green-intense-fg ansi-bold">    115</span> 

<span class="ansi-red-fg">BdbQuit</span>: </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A potential issue with this encoderâ€“decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector.  This maymake it difficult for the neural network to cope with long sentences, especially those that are longerthan the sentences in the training corpus. Choet al.(2014b) showed that indeed the performance ofa basic encoderâ€“decoder deteriorates rapidly as the length of an input sentence increases.</p>
<p>In order to address this issue, we introduce an extension to the encoderâ€“decoder model which learns to align and translate jointly.  Each time the proposed model generates a word in a translation, <strong>it(soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated.</strong>  The model then predicts a target word based on the context vectors associated withthese source positions and all the previous generated target words.</p>
<p>The most important distinguishing feature of this approach from the basic encoderâ€“decoder is that <strong>it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it en-codes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation.</strong>  This frees a neural translation model from having to squash all theinformation of a source sentence, regardless of its length, into a fixed-length vector.  We show thisallows a model to cope better with long sentences</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># S: Symbol that shows starting of decoding input</span>
<span class="c1"># E: Symbol that shows starting of decoding output</span>
<span class="c1"># P: Symbol that will fill in blank sequence if current batch data size is short than time steps</span>

<span class="k">def</span> <span class="nf">make_batch</span><span class="p">():</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_class</span><span class="p">)[[</span><span class="n">word_dict</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()]]]</span>
    <span class="n">output_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_class</span><span class="p">)[[</span><span class="n">word_dict</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()]]]</span>
    <span class="n">target_batch</span> <span class="o">=</span> <span class="p">[[</span><span class="n">word_dict</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()]]</span>

    <span class="c1"># make tensor</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">input_batch</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">output_batch</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target_batch</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Attention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_cell</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">n_class</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_cell</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">n_class</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="c1"># Linear for attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_class</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">):</span>
        <span class="n">enc_inputs</span> <span class="o">=</span> <span class="n">enc_inputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># enc_inputs: [n_step(=n_step, time step), batch_size, n_class]</span>
        <span class="n">dec_inputs</span> <span class="o">=</span> <span class="n">dec_inputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># dec_inputs: [n_step(=n_step, time step), batch_size, n_class]</span>

        <span class="c1"># enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F</span>
        <span class="c1"># enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]</span>
        <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_cell</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>

        <span class="n">trained_attn</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">enc_hidden</span>
        <span class="n">n_step</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">n_step</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_class</span><span class="p">])</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_step</span><span class="p">):</span>  <span class="c1"># each time step</span>
            <span class="c1"># dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]</span>
            <span class="c1"># hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]</span>
            <span class="n">dec_output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_cell</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">hidden</span><span class="p">)</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_att_weight</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">)</span>  <span class="c1"># attn_weights : [1, 1, n_step]</span>
            <span class="n">trained_attn</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn_weights</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

            <span class="c1"># matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">enc_outputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">dec_output</span> <span class="o">=</span> <span class="n">dec_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># dec_output : [batch_size(=1), num_directions(=1) * n_hidden]</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [1, num_directions(=1) * n_hidden]</span>
            <span class="n">model</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># make model shape [n_step, n_class]</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">trained_attn</span>

    <span class="k">def</span> <span class="nf">get_att_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">):</span>  <span class="c1"># get attention weight one &#39;dec_output&#39; with &#39;enc_outputs&#39;</span>
        <span class="n">n_step</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">)</span>
        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_step</span><span class="p">)</span>  <span class="c1"># attn_scores : [n_step]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_step</span><span class="p">):</span>
            <span class="n">attn_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_att_score</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="c1"># Normalize scores to weights in range 0 to 1</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_att_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span>  <span class="c1"># enc_outputs [batch_size, num_directions(=1) * n_hidden]</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>  <span class="c1"># score : [batch_size, n_hidden]</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dec_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">score</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># inner product make scalar value</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ich mochte ein bier P&#39;</span><span class="p">,</span> <span class="s1">&#39;S i want a beer&#39;</span><span class="p">,</span> <span class="s1">&#39;i want a beer E&#39;</span><span class="p">]</span>

<span class="n">word_list</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">word_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">word_list</span><span class="p">))</span>
<span class="n">word_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_list</span><span class="p">)}</span>
<span class="n">number_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">w</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_list</span><span class="p">)}</span>
<span class="n">n_class</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_dict</span><span class="p">)</span>  <span class="c1"># vocab list</span>

<span class="c1"># hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">input_batch</span><span class="p">,</span> <span class="n">output_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="o">=</span> <span class="n">make_batch</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/vimarshc/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  warnings.warn(&#34;dropout option adds dropout after all but last &#34;
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">output_batch</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">400</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch:&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%04d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;cost =&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{:.6f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>

    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-129-0ccea394e595&gt;:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(attn_scores).view(1, 1, -1)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.models.distilbert.modeling_distilbert</span> <span class="kn">import</span> <span class="n">MultiHeadSelfAttention</span>
<span class="kn">from</span> <span class="nn">transformers.models.distilbert.configuration_distilbert</span> <span class="kn">import</span> <span class="n">DistilBertConfig</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.models.distilbert.modeling_distilbert</span> <span class="kn">import</span> <span class="n">Embeddings</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>

<span class="n">sequence_a</span> <span class="o">=</span> <span class="s2">&quot;This is a short sequence.&quot;</span>
<span class="n">encoded_sequence_a</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sequence_a</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
<span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoded_sequence_a</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoded_sequence_a</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-201-9ce7db18a711&gt;:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  p = embeddings(torch.tensor(torch.tensor(encoded_sequence_a).view(1,8)))
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>4</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">([</span><span class="mi">101</span><span class="p">,</span> <span class="mi">8667</span><span class="p">,</span> <span class="mi">1291</span><span class="p">,</span> <span class="mi">102</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">embeddify</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
    <span class="n">_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embeddings</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">_len</span><span class="p">))),</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">embeddify</span><span class="p">(</span><span class="s1">&#39;My Name is something and else as well in 2016&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-222-4b542d131dd7&gt;:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return embeddings(torch.tensor(torch.tensor(tokenizer(text)[&#34;input_ids&#34;]).view(1,_len))), tokenizer.convert_ids_to_tokens(token_ids)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 8, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">multi_head_attn</span> <span class="o">=</span> <span class="n">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.1</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">multi_head_attn</span><span class="o">.</span><span class="n">dim</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>768</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">Chatuur_MultiHeadSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span> <span class="c1"># (bs, seq_length, dim)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(46)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     44 </span><span class="ansi-red-fg">        &#34;&#34;&#34;
</span><span class="ansi-green-fg">     45 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">import</span> pdb<span class="ansi-blue-fg">;</span>pdb<span class="ansi-blue-fg">.</span>set_trace<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 46 </span><span class="ansi-red-fg">        </span>bs<span class="ansi-blue-fg">,</span> q_length<span class="ansi-blue-fg">,</span> dim <span class="ansi-blue-fg">=</span> query<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     47 </span><span class="ansi-red-fg">        </span>k_length <span class="ansi-blue-fg">=</span> key<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     48 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert dim == self.dim, &#39;Dimensions do not match: %s input vs %s configured&#39; % (dim, self.dim)</span>

ipdb&gt; p query.size()
torch.Size([1, 10, 768])
ipdb&gt; p key.size(1)
10
ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(47)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     45 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">import</span> pdb<span class="ansi-blue-fg">;</span>pdb<span class="ansi-blue-fg">.</span>set_trace<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     46 </span><span class="ansi-red-fg">        </span>bs<span class="ansi-blue-fg">,</span> q_length<span class="ansi-blue-fg">,</span> dim <span class="ansi-blue-fg">=</span> query<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 47 </span><span class="ansi-red-fg">        </span>k_length <span class="ansi-blue-fg">=</span> key<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     48 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert dim == self.dim, &#39;Dimensions do not match: %s input vs %s configured&#39; % (dim, self.dim)</span>
<span class="ansi-green-fg">     49 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert key.size() == value.size()</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(51)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     49 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert key.size() == value.size()</span>
<span class="ansi-green-fg">     50 </span>
<span class="ansi-green-fg">---&gt; 51 </span><span class="ansi-red-fg">        </span>dim_per_head <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dim <span class="ansi-blue-fg">//</span> self<span class="ansi-blue-fg">.</span>n_heads
<span class="ansi-green-fg">     52 </span>
<span class="ansi-green-fg">     53 </span><span class="ansi-red-fg">        </span>mask_reshp <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> k_length<span class="ansi-blue-fg">)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(53)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     51 </span><span class="ansi-red-fg">        </span>dim_per_head <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dim <span class="ansi-blue-fg">//</span> self<span class="ansi-blue-fg">.</span>n_heads
<span class="ansi-green-fg">     52 </span>
<span class="ansi-green-fg">---&gt; 53 </span><span class="ansi-red-fg">        </span>mask_reshp <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> k_length<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     54 </span>
<span class="ansi-green-fg">     55 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">def</span> shape<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

ipdb&gt; p  self.dim // self.n_heads
64
ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(55)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     53 </span><span class="ansi-red-fg">        </span>mask_reshp <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> k_length<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     54 </span>
<span class="ansi-green-fg">---&gt; 55 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">def</span> shape<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">     56 </span><span class="ansi-red-fg">            </span><span class="ansi-blue-fg">&#34;&#34;&#34; separate heads &#34;&#34;&#34;</span>
<span class="ansi-green-fg">     57 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads<span class="ansi-blue-fg">,</span> dim_per_head<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(59)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     57 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads<span class="ansi-blue-fg">,</span> dim_per_head<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     58 </span>
<span class="ansi-green-fg">---&gt; 59 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">def</span> unshape<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">     60 </span><span class="ansi-red-fg">            </span><span class="ansi-blue-fg">&#34;&#34;&#34; group heads &#34;&#34;&#34;</span>
<span class="ansi-green-fg">     61 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>contiguous<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads <span class="ansi-blue-fg">*</span> dim_per_head<span class="ansi-blue-fg">)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(63)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     61 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>contiguous<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads <span class="ansi-blue-fg">*</span> dim_per_head<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     62 </span>
<span class="ansi-green-fg">---&gt; 63 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>q_lin<span class="ansi-blue-fg">(</span>query<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     64 </span><span class="ansi-red-fg">        </span>k <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>k_lin<span class="ansi-blue-fg">(</span>key<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(64)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     62 </span>
<span class="ansi-green-fg">     63 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>q_lin<span class="ansi-blue-fg">(</span>query<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 64 </span><span class="ansi-red-fg">        </span>k <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>k_lin<span class="ansi-blue-fg">(</span>key<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     66 </span>

ipdb&gt; p q.shape
torch.Size([1, 12, 10, 64])
ipdb&gt; p self.n_heads 
12
ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(65)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     63 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>q_lin<span class="ansi-blue-fg">(</span>query<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     64 </span><span class="ansi-red-fg">        </span>k <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>k_lin<span class="ansi-blue-fg">(</span>key<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     66 </span>
<span class="ansi-green-fg">     67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(67)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     66 </span>
<span class="ansi-green-fg">---&gt; 67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>

ipdb&gt; p k.shape
torch.Size([1, 12, 10, 64])
ipdb&gt;  k.transpose(2, 3).shape
torch.Size([1, 12, 64, 10])
ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(68)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     66 </span>
<span class="ansi-green-fg">     67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     70 </span><span class="ansi-red-fg">        </span>scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span>(69)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">---&gt; 69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     70 </span><span class="ansi-red-fg">        </span>scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     71 </span>

ipdb&gt; p scores.shape
torch.Size([1, 12, 10, 10])
ipdb&gt; p q.shape
torch.Size([1, 12, 10, 64])
ipdb&gt; p k.transpose(2, 3).shape
torch.Size([1, 12, 64, 10])
ipdb&gt; pscres.shape
*** NameError: name &#39;pscres&#39; is not defined
ipdb&gt; p scores.shape
torch.Size([1, 12, 10, 10])
ipdb&gt; q
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">BdbQuit</span>                                   Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-78-b2651149d526&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>w <span class="ansi-blue-fg">=</span> q<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">,</span>x<span class="ansi-blue-fg">,</span>x<span class="ansi-blue-fg">,</span>mask<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    725</span>             result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_slow_forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    726</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 727</span><span class="ansi-red-fg">             </span>result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    728</span>         for hook in itertools.chain(
<span class="ansi-green-intense-fg ansi-bold">    729</span>                 _global_forward_hooks<span class="ansi-blue-fg">.</span>values<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, query, key, value, mask, head_mask, output_attentions)</span>
<span class="ansi-green-intense-fg ansi-bold">     67</span>         q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-intense-fg ansi-bold">     68</span>         scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">---&gt; 69</span><span class="ansi-red-fg">         </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-intense-fg ansi-bold">     70</span>         scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-intense-fg ansi-bold">     71</span> 

<span class="ansi-green-fg">&lt;ipython-input-74-8f2a1170a3c9&gt;</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, query, key, value, mask, head_mask, output_attentions)</span>
<span class="ansi-green-intense-fg ansi-bold">     67</span>         q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-intense-fg ansi-bold">     68</span>         scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">---&gt; 69</span><span class="ansi-red-fg">         </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-intense-fg ansi-bold">     70</span>         scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-intense-fg ansi-bold">     71</span> 

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/bdb.py</span> in <span class="ansi-cyan-fg">trace_dispatch</span><span class="ansi-blue-fg">(self, frame, event, arg)</span>
<span class="ansi-green-intense-fg ansi-bold">     86</span>             <span class="ansi-green-fg">return</span> <span class="ansi-red-fg"># None</span>
<span class="ansi-green-intense-fg ansi-bold">     87</span>         <span class="ansi-green-fg">if</span> event <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;line&#39;</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 88</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>dispatch_line<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     89</span>         <span class="ansi-green-fg">if</span> event <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;call&#39;</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     90</span>             <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>dispatch_call<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">,</span> arg<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/bdb.py</span> in <span class="ansi-cyan-fg">dispatch_line</span><span class="ansi-blue-fg">(self, frame)</span>
<span class="ansi-green-intense-fg ansi-bold">    111</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>stop_here<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">or</span> self<span class="ansi-blue-fg">.</span>break_here<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    112</span>             self<span class="ansi-blue-fg">.</span>user_line<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 113</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>quitting<span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">raise</span> BdbQuit
<span class="ansi-green-intense-fg ansi-bold">    114</span>         <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>trace_dispatch
<span class="ansi-green-intense-fg ansi-bold">    115</span> 

<span class="ansi-red-fg">BdbQuit</span>: </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 10, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">math</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Golu&#39;</span> <span class="p">,</span><span class="s1">&#39;Gupta &#39;</span><span class="p">,</span><span class="s1">&#39;is&#39;</span> <span class="p">,</span><span class="s1">&#39;a &#39;</span><span class="p">,</span><span class="s1">&#39;bad&#39;</span> <span class="p">,</span><span class="s1">&#39;man&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">golu_vect</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="c1"># dim = 256 </span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">golu_token</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">horrible_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">z</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bad_token</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">golu_pcnt</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">bod_pcnt</span> <span class="o">=</span> <span class="mf">0.4</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">golu_pcnt</span><span class="o">*</span><span class="n">golu_vect</span>
<span class="n">bad_pcnt</span><span class="o">*</span><span class="n">bad_vect</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">]</span>
<span class="p">[]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Testing-Out-DistilBert">Testing Out DistilBert<a class="anchor-link" href="#Testing-Out-DistilBert"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">AutoModel</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;__class__&#39;,
 &#39;__delattr__&#39;,
 &#39;__dict__&#39;,
 &#39;__dir__&#39;,
 &#39;__doc__&#39;,
 &#39;__eq__&#39;,
 &#39;__format__&#39;,
 &#39;__ge__&#39;,
 &#39;__getattribute__&#39;,
 &#39;__gt__&#39;,
 &#39;__hash__&#39;,
 &#39;__init__&#39;,
 &#39;__init_subclass__&#39;,
 &#39;__le__&#39;,
 &#39;__lt__&#39;,
 &#39;__module__&#39;,
 &#39;__ne__&#39;,
 &#39;__new__&#39;,
 &#39;__reduce__&#39;,
 &#39;__reduce_ex__&#39;,
 &#39;__repr__&#39;,
 &#39;__setattr__&#39;,
 &#39;__sizeof__&#39;,
 &#39;__str__&#39;,
 &#39;__subclasshook__&#39;,
 &#39;__weakref__&#39;,
 &#39;from_config&#39;,
 &#39;from_pretrained&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForQuestionAnswering</span><span class="p">,</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="n">model_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens_pt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;This is an input example&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">tokens_pt</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>input_ids:
	tensor([[ 100,   23,   31, 7301,  634]])
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_pt</span><span class="p">)</span>
<span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
<span class="c1"># pooler_output = outputs.pooler_output</span>
<span class="c1"># print(&quot;Token wise output: {}, Pooled output: {}&quot;.format(last_hidden_state.shape, pooler_output.shape))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>DistilBertModel(
  (embeddings): Embeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer): Transformer(
    (layer): ModuleList(
      (0): TransformerBlock(
        (attention): MultiHeadSelfAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (q_lin): Linear(in_features=768, out_features=768, bias=True)
          (k_lin): Linear(in_features=768, out_features=768, bias=True)
          (v_lin): Linear(in_features=768, out_features=768, bias=True)
          (out_lin): Linear(in_features=768, out_features=768, bias=True)
        )
        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (ffn): FFN(
          (dropout): Dropout(p=0.1, inplace=False)
          (lin1): Linear(in_features=768, out_features=3072, bias=True)
          (lin2): Linear(in_features=3072, out_features=768, bias=True)
        )
        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (1): TransformerBlock(
        (attention): MultiHeadSelfAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (q_lin): Linear(in_features=768, out_features=768, bias=True)
          (k_lin): Linear(in_features=768, out_features=768, bias=True)
          (v_lin): Linear(in_features=768, out_features=768, bias=True)
          (out_lin): Linear(in_features=768, out_features=768, bias=True)
        )
        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (ffn): FFN(
          (dropout): Dropout(p=0.1, inplace=False)
          (lin1): Linear(in_features=768, out_features=3072, bias=True)
          (lin2): Linear(in_features=3072, out_features=768, bias=True)
        )
        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (2): TransformerBlock(
        (attention): MultiHeadSelfAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (q_lin): Linear(in_features=768, out_features=768, bias=True)
          (k_lin): Linear(in_features=768, out_features=768, bias=True)
          (v_lin): Linear(in_features=768, out_features=768, bias=True)
          (out_lin): Linear(in_features=768, out_features=768, bias=True)
        )
        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (ffn): FFN(
          (dropout): Dropout(p=0.1, inplace=False)
          (lin1): Linear(in_features=768, out_features=3072, bias=True)
          (lin2): Linear(in_features=3072, out_features=768, bias=True)
        )
        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (3): TransformerBlock(
        (attention): MultiHeadSelfAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (q_lin): Linear(in_features=768, out_features=768, bias=True)
          (k_lin): Linear(in_features=768, out_features=768, bias=True)
          (v_lin): Linear(in_features=768, out_features=768, bias=True)
          (out_lin): Linear(in_features=768, out_features=768, bias=True)
        )
        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (ffn): FFN(
          (dropout): Dropout(p=0.1, inplace=False)
          (lin1): Linear(in_features=768, out_features=3072, bias=True)
          (lin2): Linear(in_features=3072, out_features=768, bias=True)
        )
        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (4): TransformerBlock(
        (attention): MultiHeadSelfAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (q_lin): Linear(in_features=768, out_features=768, bias=True)
          (k_lin): Linear(in_features=768, out_features=768, bias=True)
          (v_lin): Linear(in_features=768, out_features=768, bias=True)
          (out_lin): Linear(in_features=768, out_features=768, bias=True)
        )
        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (ffn): FFN(
          (dropout): Dropout(p=0.1, inplace=False)
          (lin1): Linear(in_features=768, out_features=3072, bias=True)
          (lin2): Linear(in_features=3072, out_features=768, bias=True)
        )
        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (5): TransformerBlock(
        (attention): MultiHeadSelfAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (q_lin): Linear(in_features=768, out_features=768, bias=True)
          (k_lin): Linear(in_features=768, out_features=768, bias=True)
          (v_lin): Linear(in_features=768, out_features=768, bias=True)
          (out_lin): Linear(in_features=768, out_features=768, bias=True)
        )
        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (ffn): FFN(
          (dropout): Dropout(p=0.1, inplace=False)
          (lin1): Linear(in_features=768, out_features=3072, bias=True)
          (lin2): Linear(in_features=3072, out_features=768, bias=True)
        )
        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens_pt</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;input_ids&#39;: tensor([[ 100,   23,   31, 7301,  634]])}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens_pt</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 100,   23,   31, 7301,  634]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">tokens_pt</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;This&#39;, &#39;is&#39;, &#39;an&#39;, &#39;input&#39;, &#39;example&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 5, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">outputs</span><span class="o">.</span><span class="n">pooler_output</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-532-09dec4a5b19a&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>outputs<span class="ansi-blue-fg">.</span>pooler_output<span class="ansi-blue-fg">.</span>shape

<span class="ansi-red-fg">AttributeError</span>: &#39;BaseModelOutput&#39; object has no attribute &#39;pooler_output&#39;</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 7, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">2023</span><span class="p">,</span> <span class="mi">2003</span><span class="p">,</span> <span class="mi">2019</span><span class="p">,</span> <span class="mi">7953</span><span class="p">,</span> <span class="mi">2742</span><span class="p">,</span>  <span class="mi">102</span><span class="p">]]))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 0.0390, -0.0123, -0.0208, -0.0005, -0.0198,  0.0383, -0.0206,  0.0034,
        -0.0225, -0.0440], grad_fn=&lt;SliceBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">2023</span><span class="p">,</span> <span class="mi">2003</span><span class="p">,</span> <span class="mi">2019</span><span class="p">,</span> <span class="mi">7953</span><span class="p">,</span> <span class="mi">2742</span><span class="p">,</span>  <span class="mi">102</span><span class="p">]]))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 0.0390, -0.0123, -0.0208, -0.0005, -0.0198,  0.0383, -0.0206,  0.0034,
        -0.0225, -0.0440], grad_fn=&lt;SliceBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;input_ids&#39;: [101, 1045, 2572, 1037, 2204, 2711, 1998, 1045, 2066, 3256, 6949, 102], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="s1">&#39;I am a good person and I like ice cream&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vec</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">embeddify</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-222-4b542d131dd7&gt;:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return embeddings(torch.tensor(torch.tensor(tokenizer(text)[&#34;input_ids&#34;]).view(1,_len))), tokenizer.convert_ids_to_tokens(token_ids)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;input_ids&#39;: [101, 1045, 2572, 1037, 2204, 2711, 1998, 1045, 2066, 3256, 6949, 102], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-264-4703d73a0bd4&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>model<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">**</span>tokens<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    725</span>             result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_slow_forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    726</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 727</span><span class="ansi-red-fg">             </span>result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    728</span>         for hook in itertools.chain(
<span class="ansi-green-intense-fg ansi-bold">    729</span>                 _global_forward_hooks<span class="ansi-blue-fg">.</span>values<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)</span>
<span class="ansi-green-intense-fg ansi-bold">    700</span>         return_dict <span class="ansi-blue-fg">=</span> return_dict <span class="ansi-green-fg">if</span> return_dict <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span> <span class="ansi-green-fg">else</span> self<span class="ansi-blue-fg">.</span>config<span class="ansi-blue-fg">.</span>use_return_dict
<span class="ansi-green-intense-fg ansi-bold">    701</span> 
<span class="ansi-green-fg">--&gt; 702</span><span class="ansi-red-fg">         distilbert_output = self.distilbert(
</span><span class="ansi-green-intense-fg ansi-bold">    703</span>             input_ids<span class="ansi-blue-fg">=</span>input_ids<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">    704</span>             attention_mask<span class="ansi-blue-fg">=</span>attention_mask<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    725</span>             result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_slow_forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    726</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 727</span><span class="ansi-red-fg">             </span>result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    728</span>         for hook in itertools.chain(
<span class="ansi-green-intense-fg ansi-bold">    729</span>                 _global_forward_hooks<span class="ansi-blue-fg">.</span>values<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)</span>
<span class="ansi-green-intense-fg ansi-bold">    463</span>             <span class="ansi-green-fg">raise</span> ValueError<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;You cannot specify both input_ids and inputs_embeds at the same time&#34;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    464</span>         <span class="ansi-green-fg">elif</span> input_ids <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 465</span><span class="ansi-red-fg">             </span>input_shape <span class="ansi-blue-fg">=</span> input_ids<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    466</span>         <span class="ansi-green-fg">elif</span> inputs_embeds <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    467</span>             input_shape <span class="ansi-blue-fg">=</span> inputs_embeds<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span>

<span class="ansi-red-fg">AttributeError</span>: &#39;list&#39; object has no attribute &#39;size&#39;</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Transformer-XL-Things">Transformer XL Things<a class="anchor-link" href="#Transformer-XL-Things"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># model_checkpoint = &quot;distilbert-base-uncased&quot;</span>
<span class="c1"># model = AutoModel.from_pretrained(model_checkpoint)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_config</span> <span class="o">=</span> <span class="s1">&#39;transfo-xl-wt103&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TransfoXLConfig</span><span class="p">,</span> <span class="n">TransfoXLModel</span><span class="p">,</span> <span class="n">TransfoXLTokenizer</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">configuration</span> <span class="o">=</span> <span class="n">TransfoXLConfig</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">TransfoXLModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/vimarshc/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.
  if getattr(self, &#34;_initialized&#34;, False) and not isinstance(value, torch.nn.Parameter):
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">_config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>

</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">_config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens_pt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;This is an input example&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens_pt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;This is an input example&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens_pt</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;input_ids&#39;: tensor([[ 100,   23,   31, 7301,  634]])}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Import-and-Other-Things-I-don't-want-taking-up-space-but-ARE-IMPORTANT">Import and Other Things I don't want taking up space but ARE IMPORTANT<a class="anchor-link" href="#Import-and-Other-Things-I-don't-want-taking-up-space-but-ARE-IMPORTANT"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

