---

title: Title


keywords: fastai
sidebar: home_sidebar



nb_path: "NLP Prelims.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: NLP Prelims.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction-to-NLP">Introduction to NLP<a class="anchor-link" href="#Introduction-to-NLP"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hello Everyone! 
Welcome to the first video of this lecture series on Attention.</p>
<p>Before we start our discussion on Attention we're gonna breifly discuss how Deep Learning is used in Natural Language Processing to establish a base line of terminologies and to discuss certain concepts that form the bedrock of Deep Learning in NLP.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Like every other usage of Deep Learning models, in NLP we use different kinds of neural networks to create vectorized representations of our input. For any NLP task to create vector space representation text we first start with representing every word in our dataset with a randomly initialized vector. In PyTorch, the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html">Embedding</a> class is used to create a mapping between all words in our dataset and a fixed length vector (a tensor of single dimension). Here's an example.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># Here I&#39;ve created a mapping where my vocab size is 10 </span>
<span class="c1"># and the length of my fixed length vector is 3. </span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">9</span><span class="p">]])</span>
<span class="c1"># Each word in our dataset is given a numerical ID. </span>
<span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[-0.3612,  0.4019,  0.7331]]], grad_fn=&lt;EmbeddingBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, one of the objectives of this video was to help you the viewer develop a habit to look at source code. Keeping that in mind let's take a brief detour and see what the internals of the Embedding class look like and try to recreate sections of it.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Parameter containing:
tensor([[ 1.4227, -1.5219,  0.9717],
        [-1.1892,  0.0214, -0.5868],
        [ 2.2857,  1.0582,  0.0106],
        [-0.9125, -0.0913, -0.2314],
        [-1.4316,  0.5261, -0.6858],
        [-0.3281, -0.9034,  0.9479],
        [-1.0346,  1.0548, -0.7614],
        [-0.5806,  1.0494, -0.1831],
        [-1.4869,  0.1978,  0.6518],
        [ 1.1752, -0.7529,  1.2326]], requires_grad=True)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">weight</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Parameter containing:
tensor([[-1.6990e-26,  3.0871e-41, -2.6363e-29],
        [ 4.5593e-41,  2.1449e-02,  5.8680e-01],
        [ 2.2857e+00,  1.0582e+00,  1.0593e-02],
        [ 9.1253e-01,  9.1309e-02,  2.3141e-01],
        [ 1.4316e+00,  5.2611e-01,  6.8577e-01],
        [ 3.2810e-01,  9.0337e-01,  9.4789e-01],
        [ 1.0346e+00,  1.0548e+00,  7.6137e-01],
        [ 5.8065e-01,  1.0494e+00,  1.8311e-01],
        [ 1.4869e+00,  1.9779e-01,  6.5181e-01],
        [ 1.1752e+00,  7.5294e-01,  1.2326e+00]], requires_grad=True)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another library that we're gonna be actively looking at is the <a href="https://huggingface.co/transformers/">transformers</a> library. This is a library build on top of PyTorch which provides a lot inbuilt functionality for NLP tasks. Here, I've created the <code>embeddify</code> func that uses that library to return the vector space representations for every word passed in a sentence as input.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">embeddify</span><span class="p">(</span><span class="s1">&#39;The attention mechanism was invented in 2015&#39;</span><span class="p">)</span>
<span class="c1"># We will look at the internals of this function at some other point. </span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-222-4b542d131dd7&gt;:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return embeddings(torch.tensor(torch.tensor(tokenizer(text)[&#34;input_ids&#34;]).view(1,_len))), tokenizer.convert_ids_to_tokens(token_ids)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="c1"># (batch_size,seq_len, token_vector_size)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 7, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;The&#39;, &#39;attention&#39;, &#39;mechanism&#39;, &#39;was&#39;, &#39;invented&#39;, &#39;in&#39;, &#39;2015&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>a</code> is a 3 dimensional tensor. Lets take a look at what each of those dimensions mean:</p>
<p><strong>batch_size</strong> : This refers to the number of sentences being represented by the tensor. Usually when I'm training or evaluating a model I'm gonna be passing multiple sentences.</p>
<p><strong>seq_len</strong>: This refers to the number of words in my sentence. Here, my sentence is made up of 7 words.  Each entity is further represented by a vector of size 768.</p>
<p><strong>token_vector_size</strong>: This is the length of the fixed length vector representing every word.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-222-4b542d131dd7&gt;:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return embeddings(torch.tensor(torch.tensor(tokenizer(text)[&#34;input_ids&#34;]).view(1,_len))), tokenizer.convert_ids_to_tokens(token_ids)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;The&#39;, &#39;attention&#39;, &#39;mechanism&#39;, &#39;was&#39;, &#39;invented&#39;, &#39;in&#39;, &#39;2015&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rnn_example_hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">768</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">example_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span><span class="mi">768</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="p">,</span><span class="n">last_encoder_output</span> <span class="o">=</span> <span class="n">example_rnn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">rnn_example_hidden</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 7, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 7, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">last_encoder_output</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 1, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, you can look at a as a sequence of vectors representing our sentence. Given such input  <a href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html">RNN</a>s are used to generate a vector of fixed size. Here, <code>example_rnn</code> is an RNN when takes as input a sequence of vectors which are 768 in size and to represent the sequence of tokens uses a vector of size 768.</p>
<p>How a RNN works is that it starts off with fixed length vector, which is referred to as the hidden state (rnn_example_hidden) processes the input sequentially. At each step it performs an operation to merge a vector  with the hidden state.</p>
<p><code>last_encoder_output</code> represents the entire sequence in <code>b</code> and was created after merging the last vector/word with the hidden state. <code>encoder_outputs</code> is a list of "hidden states" generated after merging each vector of the hidden state. You can see that there are 7 entities in <code>encoder_outputs</code>.</p>
<blockquote><p><strong>encoder_outputs[0]</strong> represents the merging of <strong>_rnn_example<em>hidden</em></strong> and the word <strong><em>The</em></strong></p>
<p><strong>encoder_outputs[1]</strong> represents the merging of <strong>_encoder<em>outputs[0]</em></strong> and the word <strong><em>attention</em></strong></p>
</blockquote>
<p>So and and so forth. Lastly, encoder_outputs[-1] represents the entire sequence.</p>
<p>Now, a discussion of the internals of the RNN module is beyond the scope of this article. For a more in depth introduction to RNNs I would first suggest <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">this</a> tutorial by <a href="https://karpathy.ai/">Andrej Karpathy</a> and follow that up with <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">this</a> discussion by Chris Olah. But it's worth taking a moment to discuss one particular drawback of the RNN. The RNN does not do a very good job of representing long sequences very well and only remembers things near the corresponding input. So, a consequence of that in our case will be:</p>
<blockquote><p>encoder_outputs[-1] does a good job of remebering the information near the token <code>2015</code> but might not represent earlier tokens in the sequence very well.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Humble-Linear-Layer">The Humble Linear Layer<a class="anchor-link" href="#The-Humble-Linear-Layer"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/1layer.gif" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's talk a bit about the humble Linear Layer. Now, if any of you have trained a neural network chances are you've stacked up a few linear layers and covered them with activation functions to perform some task. However, for the longest time I did not understand the intuition behind them. The idea is subtle and it has got to do with visualizing what a linear layer does.</p>
<p>A Linear layer performs a <code>Linear Transformations</code>. Linear Transformations (though they have certain properties attached to them) perform the action of stretching and squishing space it is applied upon. The properties restrict the kind of stretching and squishing that can be done. In essence, the properties translate to prarllel lines should remain parallel.</p>
<p>A Linear Transformation maps a particular vector to a different space and it does that by mapping a vector to a space where the i-cap and j-cap of that mapped space are not (1,0) and (0,1) but a different set of vectors. These vectors are represented by the weights of our linear layer.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Here I have created a Linear Layer. </span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lin1</span><span class="o">.</span><span class="n">weight</span>
<span class="c1"># For this LT i-cap is at [-0.6971,  0.2710]</span>
<span class="c1">#          and j-cap is at [0.2710,  0.0192]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Parameter containing:
tensor([[-0.6971,  0.6101],
        [ 0.2710,  0.0192]], requires_grad=True)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lin1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">]))</span>
<span class="c1"># lin1(torch.tensor([0.,1.]))</span>
<span class="c1"># lin1(torch.tensor([1.,1.]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([-0.6971,  0.2710], grad_fn=&lt;SqueezeBackward3&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A Linear Transformation and an activation function is shown in the animation above. Three things happen here: Stretching, Sliding and Squification. The first two are performed by the linear transformation and the last is performed by a activation function.  So, when vectors are passed as input they are mapped to a different space.</p>
<p>Now, I suspect many of you might be full of questions and an exhaustive analysis of Linear Transformation is beyond the scope of this lecture. But, <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">here's</a> an excellect series by the wonderful <a href="https://twitter.com/3Blue1Brown">3Blue1Brown</a> on Linear Transformations which without a doubt will cover most of your questions. Seriously folks, it changed how I look at Deep Learning and without a doubt it will change your prespective as well. Also, I have taken the above animation from <a href="https://twitter.com/ch402">Chis Olah</a>'s wonderful <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">blogpost</a> about NNs and topology (which I would suggest you only look at after the LA course it might confuse you further)</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # lin1 = nn.Linear(2,2, bias=False)</span>
<span class="c1"># # lin2 = nn.Linear(2,2, bias=False)</span>

<span class="c1"># b = lin1(torch.tensor(a))</span>
<span class="c1"># c = lin2(torch.tensor(a))</span>

<span class="c1"># print(b)</span>
<span class="c1"># print(c)</span>

<span class="c1"># import numpy as np</span>
<span class="c1"># import matplotlib.pyplot as plt</span>

<span class="c1"># V = np.array([[.1,.1], [-.4452,.6256], [-.0827,.1850]])</span>
<span class="c1"># origin = np.array([[0, 0, 0],[0, 0, 0]]) # origin point</span>

<span class="c1"># plt.quiver(*origin, V[:,0], V[:,1], color=[&#39;r&#39;,&#39;b&#39;,&#39;g&#39;], scale=2)</span>
<span class="c1"># plt.show()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-SoftMax?-Not-Perfect">What is SoftMax? Not Perfect<a class="anchor-link" href="#What-is-SoftMax?-Not-Perfect"> </a></h2><p>Given a set of scores the Softmax func is used to output probabilities. The most common use case is that of classification. The setup in which softmax is commonly used is something like this:</p>
<p>You have a network of some sort which outputs a vector of a given size and you want to use this vector to perform classification. So, if you have 5 labels you use a Linear Layer to use this vector to create 5 scores. 
For example:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># Fixed Length Vector Representing you input</span>
<span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.3326, -0.9005, -1.0053,  0.1555,  0.1022]],
       grad_fn=&lt;AddmmBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-776-a481467c755f&gt;:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  F.softmax(y)
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.1904, 0.1079, 0.0972, 0.3103, 0.2942]], grad_fn=&lt;SoftmaxBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here I have used the Softmax to convert the scores into probabilities. Now, what does the softmax do which makes me say that these are probabilities? That is related to how the softmax computes these scores. So, all that softmax does is that it normalizes the scores given to it and one of the properties of the scores generated by SoftMax is that they will always sum to 1.</p>
<p>So, in our example, if you know that the input definitely belongs to one of the 5 classes and only one of the 5 classes softmax will normalize the scores so that the probabilities sum up to 1.</p>
<p>So, to conclude, What does Softmax do? 
Given a set of scores it will normalize the scores so that they sum to 1 and this allows us to think of the output as probabilities.</p>
<p>Now, there are a few caveats. if your input could belong to multiple classes or none of the classes the probabilities of the softmax function can be misleading.</p>

</div>
</div>
</div>
</div>
 

