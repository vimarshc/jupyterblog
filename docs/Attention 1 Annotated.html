---

title: The Attention Mechanism   


keywords: fastai
sidebar: home_sidebar



nb_path: "Attention 1 Annotated.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: Attention 1 Annotated.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">Introduction<a class="anchor-link" href="#Introduction"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hello Everyone!</p>
<p>Welcome to DeepLearningDots. This is the first video in a series about the Attention Mechanism. So, in this video we're not going to be taking a look at the internals of the attention mechanism but what we're going to try to do is really hammer home the intuition behind the attention mechanism.</p>
<p>We're going to talk about what the attention mechanism does and what the output looks like.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/Caption_Generator_Attention.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, before we look at some code let's try to use some visuals to get a feel of what the attention mechanism does.</p>
<p>Here, we have the results of a model which recieves as input an image and as output returns a caption for it. This model uses attention in the process of generating the caption. After recieving the image as input and before predicting each word the attention mechanism is being used to tell the model</p>
<p><code>Ok, to predict the next word which part of the image I should be focussing on?</code></p>
<p>Now before we move on to the next example lets talk about what I mean by the word <code>focussing</code> and how does that manifest itself in code. To tell the model what is focus on the attention mechanism is going to return a weight. This weight is going to be a number between 0 and 1 and the pixels with higher weight attached to it are supposed to be the pixels the model should focus on.</p>
<p>So, in this first example, the output of attention mechanism indicated to predict the word <code>frisbee</code> th model  should be focussing (or <em>attending</em> to) on the frisbee in the image.</p>
<p>Now, I think the last example is particularly cool. Before predicting the word <code>trees</code> its stating to focus on all the greenery in the background. Even thought the giraffe is dominating the image the attention mechanism is able to tell us what should be the area of focus.</p>
<p>Before we move on lets talk about this example again but this time lets use some technical jargon.</p>
<p>These examples are from the paper <a href="https://arxiv.org/pdf/1502.03044.pdf">Show, Attend and Tell: Neural Image CaptionGeneration with Visual Attention</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/Align_And_Translate_Attention.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This depcits the usage of attention in a model used to translate english sentences to french. In this example, the sentence on the X-axis is the input sentence and the sentence on the Y-axis. In this model the attention mechanism is being used in a similar manner to the example we just went through. To predict each word the model uses attention to state which part of the source sentence should be focussed on.</p>
<p>Now, lets focus on the word <code>en</code> in the translation. The attention mechanism is saying you should be focussing on the word <code>in</code> and I believe is more fascinating and cool is that it's also saying that to predict the word <code>en</code> some information or some amount of context is also being provided by the words <code>signed</code> and <code>August</code>.</p>
<p>This example is from the paper which invented the attention mechanism: <a href="https://arxiv.org/pdf/1409.0473v7.pdf">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I was able to tell attention till where I've predicted (the current state) in return tell us which parts of the input are important.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="I've-seen-what-attention-does-visually;-let's-take-a-step-forward">I've seen what attention does visually; let's take a step forward<a class="anchor-link" href="#I've-seen-what-attention-does-visually;-let's-take-a-step-forward"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/Align_And_Translate_Attention.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We've already seen that given a list of things the attention mechanism provides a weight for each item in the list. In this example, a list of words is given to the attention mechanism and I wanted to know which words are more important and thus should have more weight.</p>
<p>Lets take a look at what this looks like in code. But before we take a look at what the function looks like from inside let's take a look at how the attention function is used.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">HDDN_DIM</span><span class="p">)</span> <span class="c1"># hidden = entity_hidden</span>

<span class="c1"># List of Entities</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">90</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ENC_DIM</span><span class="p">)</span> <span class="c1"># encoder_outputs = [entity_1, entity_2, entity_3]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([90, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span><span class="p">,</span> <span class="n">energy</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span><span class="n">encoder_outputs</span><span class="p">)</span>

<span class="c1"># bmm </span>
<span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
    <span class="n">encoder_outputs_attention_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-1-278ca01c7e3f&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-red-fg"># Usage of the attention function.</span>
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg"> </span>encoder_outputs_attention_weights<span class="ansi-blue-fg">,</span> energy <span class="ansi-blue-fg">=</span> attention<span class="ansi-blue-fg">(</span>hidden<span class="ansi-blue-fg">,</span>encoder_outputs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> 
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span class="ansi-red-fg"># bmm</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> context_vector = torch.bmm(

<span class="ansi-red-fg">NameError</span>: name &#39;attention&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If there is an overriding objective of this video it is that you the viewer should get comfortable with the last cell and we're gonna discuss this cell at length. Here I'm using the attention function (which I've defined below and we're gonna take a look at it eventually) to calculate a set of weights where encoder_outputs is the "list of things" we saw in our visuals above.</p>
<h2 id="The-Input-">The Input <a class="anchor-link" href="#The-Input-"> </a></h2><p>Here the input variables that I have defined are randomly initialized variables but it is important to note that if we were performing NMT the tensor form in which I have initialized this variables is exactly what the actual variables would look like. So, let's take a look at what the input to the attenion mechanism is supposed to represent:</p>
<p><strong>encoder_outputs</strong> : This is the list of entities for which the weights are going to be calculated. In the context of NMT it represents the source sentence after ingesting each token. Here our source sentence has 14 tokens and encoder_outputs has 14 hidden_state vectors.</p>
<p><strong>hidden</strong>:This a fixed length vector which represents (for our NMT example) the translated sentence so far. So, if the model has outputted 3 words in french it is supposed to represent the sequence of those 3 words. But, more importantly it is supposed to tell the attention mechanism which word we want to predict next. Here, I would like to take a moment to talk about how to think about this variable in more abstract terms and I'd like to argue that the purpose of the attention mechanism is to query information from a list of vectors. When I'm translating something, I'm querying "Which part of the source sentence I should focus on to predict the next word?" or when I'm generating a caption I'm querying "Which part of the image I should focus on to predict the next word?". So, how I like to think about the <em>hidden</em> variable is that it is used as a query.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span><span class="p">,</span> <span class="n">energy</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([[0.0897, 0.0583, 0.0661, 0.0786, 0.0655, 0.0809, 0.0639, 0.0688, 0.0652,
          0.0619, 0.0821, 0.0815, 0.0630, 0.0745]], grad_fn=&lt;SoftmaxBackward&gt;),
 tensor([[ 0.1802, -0.2512, -0.1253,  0.0478, -0.1342,  0.0772, -0.1589, -0.0844,
          -0.1381, -0.1903,  0.0922,  0.0851, -0.1732, -0.0048]],
        grad_fn=&lt;SqueezeBackward1&gt;))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(1.0000, grad_fn=&lt;SumBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([50, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Output-">The Output <a class="anchor-link" href="#The-Output-"> </a></h2><p>Now, here the attention func is returning two variables but when you're actually making a model you're never really gonna need <code>energy</code>. We've already discussed how attention tells us which things to focus on in a list of entities. It does that by returning a weight for each item in my list. Higher the weight means an entity is more important. So, <code>encoder_outputs_attention_weights</code> represents weights for each item in <code>encoder_outputs</code>. Now, here's an important property for the weights attention returns. In all the implementations of attention that have been invented yet this property is gonna hold true. The weights are always gonna sum up to 1 and this gives me the opportunity to state that these scores are normalized.</p>
<p><code>energy</code> represents the unnormalized weights.</p>
<p>There's an interesting point to note here. The attention function has no dependence on the number of items in my list. Regadless of the number of items in my list it will use the query object and return a set of weights for each item in _encoder<em>outputs</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="BMM">BMM<a class="anchor-link" href="#BMM"> </a></h2><p>Now, we're at a place where I have a list of entities from which I want some information, I have a query object which I can use to fetch the information and using these two things and the attention func I have calculated a set of weights telling me which entities in my list are more important than the others. Let's see how these weights are used when I'm developing a model.</p>
<p>As we've already seen the attention func does not depend on the number of entities in _encoder<em>outputs</em> , so I need a way to use these weights that does not depend on that either. How that is done is a bit tricky and was not intuitive for me when I first went through it.</p>
<p>The objective of the attention func is to query information and in my model I want to represent the fetched information using a single fixed length vector. So, to do that I multiply each item in encoder_output with it's corresponding weight and I add all the entities together.</p>
<p>Here's an example</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([14, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_weights</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 14])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_small</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="c1"># [1.,2.]*0.7 = [0.7, 1.4]</span>
    <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span> <span class="c1"># [2.,1.]*0.2 = [0.4, 0.2]</span>
    <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]</span>  <span class="c1"># [1.,1.]*0.1 = [0.1, 0.1]</span>
<span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

<span class="n">attn_weights_small</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.7</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">]</span>
<span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_small</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([3, 1, 2])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn_weights_small</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 3])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
    <span class="n">attn_weights_small</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">encoder_outputs_small</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="c1"># This vector is supposed be to 70% of [1., 2.] and 20% of [2., 1.]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[1.2000, 1.7000]]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The context vector obtained is supposed to be a vector representation of:</p>
<ol>
<li>The image with the focus on the frisbee from the first example. </li>
<li>The source sentence with the focus on the word 'in' in the NMT example. </li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Internals-of-the-Attention-Mechanism">Internals of the Attention Mechanism<a class="anchor-link" href="#Internals-of-the-Attention-Mechanism"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ok. After a lot of effort we're at a stage where we can start looking at the internals of the attention function. But I believe if you've been able to follow this lecture so far this next section should be a breeze.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># DEC_HID_DIM = 5</span>

<span class="n">HDDN_DIM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">ENC_DIM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">DEC_DIM</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HDDN_DIM</span> <span class="o">+</span> <span class="n">ENC_DIM</span><span class="p">,</span> <span class="n">DEC_DIM</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">DEC_DIM</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Linear(in_features=10, out_features=5, bias=True)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">HDDN_DIM</span><span class="p">)</span> <span class="c1"># hidden = entity_hidden</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ENC_DIM</span><span class="p">)</span> <span class="c1"># encoder_outputs = [entity_1, entity_2, entity_3]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span> <span class="c1">#(seq_len, batchsize, vector_size)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([10, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span><span class="n">encoder_outputs</span><span class="p">):</span>
<span class="c1">#     import pdb;pdb.set_trace()</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">src_len</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1">#repeat decoder hidden state src_len times</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># energy function </span>
    <span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
        <span class="n">attn</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">(</span><span class="n">hidden</span><span class="p">,</span>
                <span class="n">encoder_outputs</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span> 
    <span class="n">attention</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">energy</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">attention</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Walking-Back-">Walking Back <a class="anchor-link" href="#Walking-Back-"> </a></h2><p>You should be familiar with the cell below as we've just discussed this. Let's try to walk back from what we know of this cell to what we can deduce without actually looking at the internals. We have a query object <code>hidden</code>, a list of entities <code>encoder_outputs</code>. The attention func is gonna return a set of normalized weights for each item in my list.</p>
<p>Now, I think we can deduce the following from this information that we have with us: 
We're going to calculate some score by performing <strong>some operation</strong> between the query object (<code>hidden</code>) and each item in <code>encoder_outputs</code> and after calculating all the scores we're going to normalize the scores so that they'll fullfill the property of summing up to 1.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span><span class="p">,</span> <span class="n">energy</span>  <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

<span class="c1"># bmm </span>
<span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
    <span class="n">encoder_outputs_attention_softmax_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The probability $\alpha_{ij}$ , or its associated energy $e_{ij}$, reflects the importance of the annotation $h_{j}$ with respect to the previous hidden states $iâˆ’1$ in deciding the next states $i$ and generating $y_i$ . Intuitively,this implements a mechanism of attention in the decoder.  The decoder decides parts of the source sentence to pay attention to.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">HDDN_DIM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">ENC_DIM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">DEC_DIM</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HDDN_DIM</span> <span class="o">+</span> <span class="n">ENC_DIM</span><span class="p">,</span> <span class="n">DEC_DIM</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">DEC_DIM</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">HDDN_DIM</span><span class="p">)</span> <span class="c1"># hidden = entity_hidden</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ENC_DIM</span><span class="p">)</span> <span class="c1"># encoder_outputs = [entity_1, entity_2, entity_3]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Attention-Function">The Attention Function<a class="anchor-link" href="#The-Attention-Function"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's zoom into the attention func. I've taken every line and put it in a different cell.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([10, 1, 5])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">src_len</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Since I want to calculate a score by performing some operation between each item in `encoder_outputs` </span>
<span class="c1"># and the query object I&#39;m gonna need a copies of the query object equal to the number of items in </span>
<span class="c1"># `encoder_outputs`</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
    <span class="n">attn</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">(</span><span class="n">hidden</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span> 

<span class="n">attention</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">energy</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.0732, 0.0731, 0.0642, 0.0703, 0.0646, 0.0657, 0.0823, 0.0777, 0.0837,
         0.0767, 0.0706, 0.0537, 0.0651, 0.0790]], grad_fn=&lt;SoftmaxBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-Energy-Function">The Energy Function<a class="anchor-link" href="#The-Energy-Function"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/energy_functions.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, a question worth asking here is is "Is the Energy Function" defined above special? Are there others forms to it? The answer is Yes. The picture describes a few other forms of the attention function. I believe the only limitation to the Energy Function is that I should be able to backpropogate through it. So, let's see another form of the energy function in the example below.</p>
<p>I have taken this image from <a href="https://arxiv.org/pdf/1508.04025.pdf">Effective Approaches to Attention-based Neural Machine Translation</a>. Which further introduces another kind of energy function. I'm not going to go into details here but it introduces a "local" attention mechanism. This mechanism instead of taking as input all the words from our NMT example above, takes a lesser number of entities. I believe the NMT <a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">implementation</a> available on the PyTorch website uses that implementation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="General-Attention">General Attention<a class="anchor-link" href="#General-Attention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_att_weight</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">):</span>  <span class="c1"># get attention weight one &#39;dec_output&#39; with &#39;enc_outputs&#39;</span>
    <span class="n">n_step</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">)</span>
    <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_step</span><span class="p">)</span>  <span class="c1"># attn_scores : [n_step]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_step</span><span class="p">):</span>
        <span class="n">attn_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_att_score</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="c1"># Normalize scores to weights in range 0 to 1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">attn_scores</span>

<span class="k">def</span> <span class="nf">get_att_score</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span>  <span class="c1"># enc_outputs [batch_size, num_directions(=1) * n_hidden]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>  <span class="c1"># score : [batch_size, n_hidden]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dec_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">score</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># inner product make scalar value</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span><span class="p">,</span> <span class="n">energy</span> <span class="o">=</span> <span class="n">get_att_weight</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-152-198907164ce3&gt;:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(attn_scores).view(1, 1, -1), attn_scores
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder_outputs_attention_softmax_weights</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[3.4176e-04, 8.2757e-01, 1.7209e-01]]], grad_fn=&lt;ViewBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">energy</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([-5.8196,  1.9725,  0.4020], grad_fn=&lt;CopySlices&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Further-Reading">Further Reading<a class="anchor-link" href="#Further-Reading"> </a></h2><p>Before we move on to the next topic we've covered enough ground here for you to be able to look at implementations of the attention mechanisms and/or read research papers that utilize the attention mechanism for various tasks.</p>
<p><a href="https://arxiv.org/pdf/1409.0473v7.pdf"><strong>NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</strong></a>: This paper introduced the attention mechanism and it did that in the context of the NMT task. <a href="https://paperswithcode.com/paper/neural-machine-translation-by-jointly">Here</a> are the implementations of this paper on paperwithcode.</p>
<p><a href="https://arxiv.org/pdf/1508.04025.pdf"><strong>Effective Approaches to Attention-based Neural Machine Translation</strong></a>: This paper introduces a new energy function as I discussed above.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Self-Attention">Self-Attention<a class="anchor-link" href="#Self-Attention"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In my opinion the attention mechanism was an important discovery because when coupled with RNNs and CNNs they improved upon the SOTA results. However, self-attention is for a reason a little different than that. In 2017, the researchers at Google published a research paper detailing a new architecture that they had invented and this architecture had self-attention as a core component. This architecture is called <code>The Transformer</code>. The invention of this architecture was something of a watershed moment (even though it feels like they happen every other day in the Deep Learning field). This standalone model is really powerful and the performance of this models on a whole host NLP tasks has been truly awesome.</p>
<p>Since 2017 a large amount of research has be done to improve upon the Transformer model. The number of such research papers published has been so large that HugginFace created a library called <a href="https://huggingface.co/transformers/">transformers</a>. This library has the implementations of a large number of research paper which can be utilized for NLP tasks.</p>
<p>To explore self-attention we're going to be using this library and we will try to decode the implementation of self-attention in this library.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.models.distilbert.modeling_distilbert</span> <span class="kn">import</span> <span class="n">MultiHeadSelfAttention</span>
<span class="kn">from</span> <span class="nn">transformers.models.distilbert.configuration_distilbert</span> <span class="kn">import</span> <span class="n">DistilBertConfig</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>PreTrainedTokenizerFast(name_or_path=&#39;distilbert-base-uncased&#39;, vocab_size=30522, model_max_len=512, is_fast=True, padding_side=&#39;right&#39;, special_tokens={&#39;unk_token&#39;: &#39;[UNK]&#39;, &#39;sep_token&#39;: &#39;[SEP]&#39;, &#39;pad_token&#39;: &#39;[PAD]&#39;, &#39;cls_token&#39;: &#39;[CLS]&#39;, &#39;mask_token&#39;: &#39;[MASK]&#39;})</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">embeddify</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
    <span class="n">_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embeddings</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">_len</span><span class="p">))),</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">multi_head_attn</span> <span class="o">=</span> <span class="n">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embeddified_text</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">embeddify</span><span class="p">(</span><span class="s1">&#39;Ronaldo is one of the best football players in the world&#39;</span><span class="p">)</span>
<span class="c1"># x = torch.randn(1,10,config.dim) # (bs, seq_length, dim)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-795-4b542d131dd7&gt;:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return embeddings(torch.tensor(torch.tensor(tokenizer(text)[&#34;input_ids&#34;]).view(1,_len))), tokenizer.convert_ids_to_tokens(token_ids)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embeddified_text</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 14, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;[CLS]&#39;,
 &#39;ronald&#39;,
 &#39;##o&#39;,
 &#39;is&#39;,
 &#39;one&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;best&#39;,
 &#39;football&#39;,
 &#39;players&#39;,
 &#39;in&#39;,
 &#39;the&#39;,
 &#39;world&#39;,
 &#39;[SEP]&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">14</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">multi_head_attn_op</span> <span class="o">=</span> <span class="n">multi_head_attn</span><span class="p">(</span>
    <span class="n">embeddified_text</span><span class="p">,</span>
    <span class="n">embeddified_text</span><span class="p">,</span>
    <span class="n">embeddified_text</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">multi_head_attn_op</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>1</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">multi_head_attn_op</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 14, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I realize I have implemented a lot of code here but I believe most of it should not be that scary. Hugginface has also created a library for Tokenization. For NLP tasks prastices have been developed that keeping all other things the same give better results. The Tokenizer library has encapsulated those practices.</p>
<p>In this demo I am using the MultiHeadAttention class that is being used in the DistillBert implementation. DistilBert is a improvement on top of the Transformer architecture.</p>
<p>The embeddify func has retuned a tensor creating a vector space representation for each token in a sentence and I've passed that tensor as input to MultiHeadAttention. As output it seems like it has returned a vector for each entry in my input. Let's check out a visual representation of what happened here:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/jupyterblog/assets/transformer_self-attention_visualization.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, someone when discovering self-attention looked at some version the <code>embeddified_text</code> tensor representing a sequence of entities where each entity is represented by a fixed length vector tried the following:</p>
<p>They wanted to use attention to use each entity to extract information from itself. So, here the vector representing each word is treated as the query object and attention is being used to fetch a set of weights for the entities in itself. I hope why this is called self-attention is clear now. We can now also decode what the output <code>multi_head_attn_op[0]</code> is.</p>
<p>So, for every entity:</p>
<ol>
<li>I implement a energy function where the entity is the query and the "list of things" is itself. </li>
<li>I will apply SoftMax on the weights on the output of the energy func. </li>
<li>I'm gonna multiply the weights with each vector and add them together to get a context vector like before. </li>
</ol>
<p>So, for each entity in my sequence I will get a context vector. The weightage being depicted in our visual above is supposed to represent the softmaxed weights where the word 'it' was the query and using the weights we will get a context vector. Now, what does the weights tell us? Its telling us that the word 'it' has a strong relationship with the words 'The animal' as compared to the word 'because' and if we read the sentence that kinda makes sense.</p>
<p>Before we start looking at how this is implemented a note on what MultiHead means. MultiHead means that instead of having one vector representing each word,we will have one word being represented by multiple vectors. This will lead to attention mechanism will be applied multiple times in parallel and multiple context vectors being created. Best to explain it further by code:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">chatuur_multi_head_attn</span> <span class="o">=</span> <span class="n">Chatuur_MultiHeadSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">op</span> <span class="o">=</span> <span class="n">chatuur_multi_head_attn</span><span class="p">(</span><span class="n">embeddified_text</span><span class="p">,</span>
                             <span class="n">embeddified_text</span><span class="p">,</span>
                             <span class="n">embeddified_text</span><span class="p">,</span>
                             <span class="n">mask</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(46)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     44 </span><span class="ansi-red-fg">        &#34;&#34;&#34;
</span><span class="ansi-green-fg">     45 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">import</span> pdb<span class="ansi-blue-fg">;</span>pdb<span class="ansi-blue-fg">.</span>set_trace<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 46 </span><span class="ansi-red-fg">        </span>bs<span class="ansi-blue-fg">,</span> q_length<span class="ansi-blue-fg">,</span> dim <span class="ansi-blue-fg">=</span> query<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     47 </span><span class="ansi-red-fg">        </span>k_length <span class="ansi-blue-fg">=</span> key<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     48 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert dim == self.dim, &#39;Dimensions do not match: %s input vs %s configured&#39; % (dim, self.dim)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(47)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     45 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">import</span> pdb<span class="ansi-blue-fg">;</span>pdb<span class="ansi-blue-fg">.</span>set_trace<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     46 </span><span class="ansi-red-fg">        </span>bs<span class="ansi-blue-fg">,</span> q_length<span class="ansi-blue-fg">,</span> dim <span class="ansi-blue-fg">=</span> query<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 47 </span><span class="ansi-red-fg">        </span>k_length <span class="ansi-blue-fg">=</span> key<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     48 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert dim == self.dim, &#39;Dimensions do not match: %s input vs %s configured&#39; % (dim, self.dim)</span>
<span class="ansi-green-fg">     49 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert key.size() == value.size()</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(51)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     49 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># assert key.size() == value.size()</span>
<span class="ansi-green-fg">     50 </span>
<span class="ansi-green-fg">---&gt; 51 </span><span class="ansi-red-fg">        </span>dim_per_head <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dim <span class="ansi-blue-fg">//</span> self<span class="ansi-blue-fg">.</span>n_heads
<span class="ansi-green-fg">     52 </span>
<span class="ansi-green-fg">     53 </span><span class="ansi-red-fg">        </span>mask_reshp <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> k_length<span class="ansi-blue-fg">)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(53)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     51 </span><span class="ansi-red-fg">        </span>dim_per_head <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dim <span class="ansi-blue-fg">//</span> self<span class="ansi-blue-fg">.</span>n_heads
<span class="ansi-green-fg">     52 </span>
<span class="ansi-green-fg">---&gt; 53 </span><span class="ansi-red-fg">        </span>mask_reshp <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> k_length<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     54 </span>
<span class="ansi-green-fg">     55 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">def</span> shape<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(55)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     53 </span><span class="ansi-red-fg">        </span>mask_reshp <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> k_length<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     54 </span>
<span class="ansi-green-fg">---&gt; 55 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">def</span> shape<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">     56 </span><span class="ansi-red-fg">            </span><span class="ansi-blue-fg">&#34;&#34;&#34; separate heads &#34;&#34;&#34;</span>
<span class="ansi-green-fg">     57 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads<span class="ansi-blue-fg">,</span> dim_per_head<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(59)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     57 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads<span class="ansi-blue-fg">,</span> dim_per_head<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     58 </span>
<span class="ansi-green-fg">---&gt; 59 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">def</span> unshape<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">     60 </span><span class="ansi-red-fg">            </span><span class="ansi-blue-fg">&#34;&#34;&#34; group heads &#34;&#34;&#34;</span>
<span class="ansi-green-fg">     61 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>contiguous<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads <span class="ansi-blue-fg">*</span> dim_per_head<span class="ansi-blue-fg">)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(63)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     61 </span><span class="ansi-red-fg">            </span><span class="ansi-green-fg">return</span> x<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>contiguous<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>bs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>n_heads <span class="ansi-blue-fg">*</span> dim_per_head<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">     62 </span>
<span class="ansi-green-fg">---&gt; 63 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>q_lin<span class="ansi-blue-fg">(</span>query<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     64 </span><span class="ansi-red-fg">        </span>k <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>k_lin<span class="ansi-blue-fg">(</span>key<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(64)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     62 </span>
<span class="ansi-green-fg">     63 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>q_lin<span class="ansi-blue-fg">(</span>query<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 64 </span><span class="ansi-red-fg">        </span>k <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>k_lin<span class="ansi-blue-fg">(</span>key<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     66 </span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(65)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     63 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>q_lin<span class="ansi-blue-fg">(</span>query<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     64 </span><span class="ansi-red-fg">        </span>k <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>k_lin<span class="ansi-blue-fg">(</span>key<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     66 </span>
<span class="ansi-green-fg">     67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>

ipdb&gt; 
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(67)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     65 </span><span class="ansi-red-fg">        </span>v <span class="ansi-blue-fg">=</span> shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>v_lin<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, k_length, dim_per_head)</span>
<span class="ansi-green-fg">     66 </span>
<span class="ansi-green-fg">---&gt; 67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>

ipdb&gt; p q.shape
torch.Size([1, 12, 14, 64])
ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(68)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     66 </span>
<span class="ansi-green-fg">     67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     70 </span><span class="ansi-red-fg">        </span>scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>

ipdb&gt; p k.transpose(2, 3).shape
torch.Size([1, 12, 64, 14])
ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(69)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     67 </span><span class="ansi-red-fg">        </span>q <span class="ansi-blue-fg">=</span> q <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>dim_per_head<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">---&gt; 69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     70 </span><span class="ansi-red-fg">        </span>scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     71 </span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(70)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     68 </span><span class="ansi-red-fg">        </span>scores <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     69 </span><span class="ansi-red-fg">        </span>mask <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>mask <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>view<span class="ansi-blue-fg">(</span>mask_reshp<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>expand_as<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">---&gt; 70 </span><span class="ansi-red-fg">        </span>scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     71 </span>
<span class="ansi-green-fg">     72 </span><span class="ansi-red-fg">        </span>weights <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>Softmax<span class="ansi-blue-fg">(</span>dim<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>

ipdb&gt; p scores.shape
torch.Size([1, 12, 14, 14])
ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(72)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     70 </span><span class="ansi-red-fg">        </span>scores<span class="ansi-blue-fg">.</span>masked_fill_<span class="ansi-blue-fg">(</span>mask<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span>float<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;inf&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     71 </span>
<span class="ansi-green-fg">---&gt; 72 </span><span class="ansi-red-fg">        </span>weights <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>Softmax<span class="ansi-blue-fg">(</span>dim<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     73 </span><span class="ansi-red-fg">        </span>weights <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dropout<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     74 </span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(73)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     71 </span>
<span class="ansi-green-fg">     72 </span><span class="ansi-red-fg">        </span>weights <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>Softmax<span class="ansi-blue-fg">(</span>dim<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">---&gt; 73 </span><span class="ansi-red-fg">        </span>weights <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dropout<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, k_length)</span>
<span class="ansi-green-fg">     74 </span>
<span class="ansi-green-fg">     75 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># Mask heads if we want to</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(76)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     74 </span>
<span class="ansi-green-fg">     75 </span><span class="ansi-red-fg">        </span><span class="ansi-red-fg"># Mask heads if we want to</span>
<span class="ansi-green-fg">---&gt; 76 </span><span class="ansi-red-fg">        </span><span class="ansi-green-fg">if</span> head_mask <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">     77 </span><span class="ansi-red-fg">            </span>weights <span class="ansi-blue-fg">=</span> weights <span class="ansi-blue-fg">*</span> head_mask
<span class="ansi-green-fg">     78 </span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(79)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     77 </span><span class="ansi-red-fg">            </span>weights <span class="ansi-blue-fg">=</span> weights <span class="ansi-blue-fg">*</span> head_mask
<span class="ansi-green-fg">     78 </span>
<span class="ansi-green-fg">---&gt; 79 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">,</span> v<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">     80 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> unshape<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-fg">     81 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>out_lin<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>

ipdb&gt; n
&gt; <span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span>(80)<span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">     78 </span>
<span class="ansi-green-fg">     79 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">,</span> v<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 80 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> unshape<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-fg">     81 </span><span class="ansi-red-fg">        </span>context <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>out_lin<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-fg">     82 </span>

ipdb&gt; p context.shape
torch.Size([1, 12, 14, 64])
ipdb&gt; p weights.shape
torch.Size([1, 12, 14, 14])
ipdb&gt; v.shape
torch.Size([1, 12, 14, 64])
ipdb&gt; q
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">BdbQuit</span>                                   Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-821-40fe8d92a2d0&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> op = chatuur_multi_head_attn(embeddified_text,
</span><span class="ansi-green-intense-fg ansi-bold">      2</span>                              embeddified_text<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>                              embeddified_text<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>                              mask)

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    725</span>             result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_slow_forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    726</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 727</span><span class="ansi-red-fg">             </span>result <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>forward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    728</span>         for hook in itertools.chain(
<span class="ansi-green-intense-fg ansi-bold">    729</span>                 _global_forward_hooks<span class="ansi-blue-fg">.</span>values<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, query, key, value, mask, head_mask, output_attentions)</span>
<span class="ansi-green-intense-fg ansi-bold">     78</span> 
<span class="ansi-green-intense-fg ansi-bold">     79</span>         context <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">,</span> v<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 80</span><span class="ansi-red-fg">         </span>context <span class="ansi-blue-fg">=</span> unshape<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-intense-fg ansi-bold">     81</span>         context <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>out_lin<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-intense-fg ansi-bold">     82</span> 

<span class="ansi-green-fg">&lt;ipython-input-819-8f2a1170a3c9&gt;</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, query, key, value, mask, head_mask, output_attentions)</span>
<span class="ansi-green-intense-fg ansi-bold">     78</span> 
<span class="ansi-green-intense-fg ansi-bold">     79</span>         context <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>weights<span class="ansi-blue-fg">,</span> v<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, n_heads, q_length, dim_per_head)</span>
<span class="ansi-green-fg">---&gt; 80</span><span class="ansi-red-fg">         </span>context <span class="ansi-blue-fg">=</span> unshape<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-intense-fg ansi-bold">     81</span>         context <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>out_lin<span class="ansi-blue-fg">(</span>context<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># (bs, q_length, dim)</span>
<span class="ansi-green-intense-fg ansi-bold">     82</span> 

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/bdb.py</span> in <span class="ansi-cyan-fg">trace_dispatch</span><span class="ansi-blue-fg">(self, frame, event, arg)</span>
<span class="ansi-green-intense-fg ansi-bold">     86</span>             <span class="ansi-green-fg">return</span> <span class="ansi-red-fg"># None</span>
<span class="ansi-green-intense-fg ansi-bold">     87</span>         <span class="ansi-green-fg">if</span> event <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;line&#39;</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 88</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>dispatch_line<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     89</span>         <span class="ansi-green-fg">if</span> event <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;call&#39;</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     90</span>             <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>dispatch_call<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">,</span> arg<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.8/bdb.py</span> in <span class="ansi-cyan-fg">dispatch_line</span><span class="ansi-blue-fg">(self, frame)</span>
<span class="ansi-green-intense-fg ansi-bold">    111</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>stop_here<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">or</span> self<span class="ansi-blue-fg">.</span>break_here<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    112</span>             self<span class="ansi-blue-fg">.</span>user_line<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 113</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>quitting<span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">raise</span> BdbQuit
<span class="ansi-green-intense-fg ansi-bold">    114</span>         <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>trace_dispatch
<span class="ansi-green-intense-fg ansi-bold">    115</span> 

<span class="ansi-red-fg">BdbQuit</span>: </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Chatuur_MultiHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">)</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">q_lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads</span><span class="p">):</span>
        <span class="n">attention_head_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">heads</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">find_pruneable_heads_and_indices</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">attention_head_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">)</span>
        <span class="c1"># Prune linear layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_lin</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lin</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_lin</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_lin</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_lin</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_lin</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_lin</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_lin</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Update hyper params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">attention_head_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters:</span>
<span class="sd">            query: torch.tensor(bs, seq_length, dim)</span>
<span class="sd">            key: torch.tensor(bs, seq_length, dim)</span>
<span class="sd">            value: torch.tensor(bs, seq_length, dim)</span>
<span class="sd">            mask: torch.tensor(bs, seq_length)</span>

<span class="sd">        Returns:</span>
<span class="sd">            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,</span>
<span class="sd">            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">pdb</span><span class="p">;</span><span class="n">pdb</span><span class="o">.</span><span class="n">set_trace</span><span class="p">()</span>
        <span class="n">bs</span><span class="p">,</span> <span class="n">q_length</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">k_length</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># assert dim == self.dim, &#39;Dimensions do not match: %s input vs %s configured&#39; % (dim, self.dim)</span>
        <span class="c1"># assert key.size() == value.size()</span>

        <span class="n">dim_per_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>

        <span class="n">mask_reshp</span> <span class="o">=</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k_length</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot; separate heads &quot;&quot;&quot;</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">dim_per_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">unshape</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot; group heads &quot;&quot;&quot;</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="n">dim_per_head</span><span class="p">)</span>

        <span class="c1"># query object </span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lin</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>  <span class="c1"># (bs, n_heads, q_length, dim_per_head)</span>
        
        <span class="c1"># list of things</span>
        <span class="c1"># Discuss the rearrangement for multi heads. </span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_lin</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>  <span class="c1"># (bs, n_heads, k_length, dim_per_head)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_lin</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>  <span class="c1"># (bs, n_heads, k_length, dim_per_head)</span>

        <span class="c1"># Attention All you Need paper states tha this operation improves results. </span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dim_per_head</span><span class="p">)</span>  <span class="c1"># (bs, n_heads, q_length, dim_per_head)</span>
        
        
        <span class="c1"># Dot Energy function. </span>
        <span class="c1"># show we have a score for each word being treated as query </span>
        <span class="c1"># and performing attention on itself. </span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># (bs, n_heads, q_length, k_length)</span>
        

        
        <span class="c1"># Will talk about this later</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">mask_reshp</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>  <span class="c1"># (bs, n_heads, q_length, k_length)</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>  <span class="c1"># (bs, n_heads, q_length, k_length)</span>

        <span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">scores</span><span class="p">)</span>  <span class="c1"># (bs, n_heads, q_length, k_length)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>  <span class="c1"># (bs, n_heads, q_length, k_length)</span>

        <span class="c1"># Mask heads if we want to</span>
        <span class="k">if</span> <span class="n">head_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">*</span> <span class="n">head_mask</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># (bs, n_heads, q_length, dim_per_head)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">unshape</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>  <span class="c1"># (bs, q_length, dim)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_lin</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>  <span class="c1"># (bs, q_length, dim)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">context</span><span class="p">,)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, I am leaving a few questions for the next video:</p>
<ol>
<li>The purpose of q_lin, k_lin and v_lin. </li>
<li>What is a mask. </li>
</ol>

</div>
</div>
</div>
</div>
 

